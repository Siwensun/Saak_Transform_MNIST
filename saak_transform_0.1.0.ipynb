{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from math import log\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "from skimage.transform import resize\n",
    "from skimage.util import *\n",
    "\n",
    "#load MNIST\n",
    "(X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     18,
     36,
     45,
     51,
     62,
     67,
     85,
     96,
     109,
     126,
     131,
     136,
     165,
     172,
     179,
     186,
     193,
     203,
     210,
     222,
     234,
     244,
     252,
     264
    ]
   },
   "outputs": [],
   "source": [
    "def image_set_preprocessing(X, y, batch_ratio = 1, pad_size=2, pad_method='constant'):\n",
    "    X_pad = np.pad(X, ((0,0),(pad_size,pad_size),(pad_size,pad_size)), 'constant')\n",
    "    \n",
    "    batch_size = (int)(X_pad.shape[0]*batch_ratio)\n",
    "    \n",
    "    #order = np.array(range(X_pad.shape[0]))\n",
    "    #np.random.shuffle(order)\n",
    "    #X_pad_shuffle = X_pad[order]\n",
    "    #y_shuffle = y[order]\n",
    "    X_pad_shuffle = X_pad\n",
    "    y_shuffle = y\n",
    "\n",
    "    X_train_batch = ((X_pad_shuffle[0:batch_size, :, :]).astype('float32'))/255\n",
    "    y_train_batch = y_shuffle[0:batch_size,]\n",
    "    X_train_batch = X_train_batch.reshape(X_train_batch.shape[0], X_train_batch.shape[1], X_train_batch.shape[2], 1)\n",
    "    \n",
    "    return X_train_batch, y_train_batch\n",
    "\n",
    "def image_set_preprocessing_v2(X, y, batch_ratio = 1, pad_size=2, pad_method='constant'):\n",
    "    X_pad = np.pad(X, ((0,0),(pad_size,pad_size),(pad_size,pad_size)), 'constant')\n",
    "    \n",
    "    batch_size = (int)(X_pad.shape[0]*batch_ratio)\n",
    "    \n",
    "    order = np.array(range(X_pad.shape[0]))\n",
    "    np.random.shuffle(order)\n",
    "    X_pad_shuffle = X_pad[order]\n",
    "    y_shuffle = y[order]\n",
    "\n",
    "    X_train_batch = ((X_pad_shuffle[0:batch_size, :, :]).astype('float32'))/255\n",
    "    for i in range(X_train_batch.shape[0]):\n",
    "        X_train_batch[i,:,:] = random_noise(X_train_batch[i,:,:], clip=True)\n",
    "    y_train_batch = y_shuffle[0:batch_size,]\n",
    "    X_train_batch = X_train_batch.reshape(X_train_batch.shape[0], X_train_batch.shape[1], X_train_batch.shape[2], 1)\n",
    "    \n",
    "    return X_train_batch, y_train_batch\n",
    "\n",
    "def flatten_patches(cubio, size):\n",
    "    window_shape = (1,size,size,cubio.shape[3])\n",
    "    step = (1,size,size,cubio.shape[3])\n",
    "    patches = view_as_windows(cubio, window_shape, step)\n",
    "    patches = patches.squeeze(axis = (3,4))\n",
    "    patches_panel = patches.reshape(-1, patches.shape[-3]*patches.shape[-2]*patches.shape[-1])\n",
    "    \n",
    "    return patches_panel\n",
    "\n",
    "def remove_low_variance(patches_panel,thr=0.05):\n",
    "    var = np.var(patches_panel, axis = 1)\n",
    "    patches_clean = patches_panel[var>thr]\n",
    "    \n",
    "    return patches_clean\n",
    "\n",
    "def remove_patches_mean(patches):\n",
    "    mean = patches.mean(axis = 1)\n",
    "    patches_mean_remov = (patches.T-mean).T\n",
    "    \n",
    "    return patches_mean_remov\n",
    "\n",
    "def pca_kernel(patches, n_comps, kernel):\n",
    "    pca = PCA(n_components = n_comps)\n",
    "    pca.fit(patches)\n",
    "    kernel.append(pca)  \n",
    "    \n",
    "def pca_kernel_v2(patches, kernel):\n",
    "    pca = PCA()\n",
    "    pca.fit(patches)\n",
    "    kernel.append(pca)\n",
    "    \n",
    "def pca_kernel_v3(patches, n_comps, kernel):\n",
    "    pca = KernelPCA(n_components = n_comps)\n",
    "    pca.fit(patches)\n",
    "    kernel.append(pca)\n",
    "    \n",
    "def pca_transform(kernel, layer, patches_mean_remov, patches_panel, X_train):\n",
    "    n_sample = X_train.shape[0]\n",
    "    h = (int)(X_train.shape[1]/pow(2,layer))\n",
    "    patches_proj = k[layer-1].transform(patches_mean_remov)\n",
    "    cubio_pos = patches_proj.reshape(n_sample,h,h,-1)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=3)\n",
    "    dc = patches_panel.mean(axis=1)*2\n",
    "    dc = dc.reshape(n_sample, h, h, -1)\n",
    "    cubio_next = np.concatenate((cubio_pca, dc), axis=3)\n",
    "    \n",
    "    return cubio_next\n",
    "\n",
    "def pca_transform_v2(kernel, layer, patches_mean_remov, patches_panel, X_train):\n",
    "    n_sample = X_train.shape[0]\n",
    "    h = (int)(X_train.shape[1]/pow(2,layer))\n",
    "    patches_proj = k[layer-1].transform(patches_mean_remov)\n",
    "    cubio_pos = patches_proj.reshape(n_sample,h,h,-1)\n",
    "    dc = patches_panel.mean(axis=1)*2\n",
    "    dc = dc.reshape(n_sample, h, h, -1)\n",
    "    cubio_next = np.concatenate((cubio_pos, dc), axis=3)\n",
    "    \n",
    "    return cubio_next\n",
    "\n",
    "def pca_transform_complete(kernel, layer, patches_mean_remov, patches_panel, X_train):\n",
    "    n_sample = X_train.shape[0]\n",
    "    h = (int)(X_train.shape[1]/pow(2,layer))\n",
    "    w = pow(2,layer)\n",
    "    patches_proj = k[layer-1].transform(patches_mean_remov)\n",
    "    cubio_pos = patches_proj.reshape(n_sample,h,h,w,w,-1)\n",
    "    Swap_cubio = np.swapaxes(cubio_pos, 2, 3)\n",
    "    cubio_pos = Swap_cubio.reshape(n_sample,32,32,-1)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=3)\n",
    "    \n",
    "    return cubio_pca\n",
    "\n",
    "def dc_add(X_train, layer, cubio, cubio_pca):\n",
    "    n_sample = 60000\n",
    "    cubio_enlarge = np.zeros((n_sample,32*pow(2,1),32*pow(2,1),cubio.shape[3]))\n",
    "    for i in range(n_sample):\n",
    "        cubio_enlarge[i,:,:,:] = resize(cubio[i,:,:,:], output_shape=[cubio.shape[1]*2,cubio.shape[2]*2,cubio.shape[3]])\n",
    "    window_shape = (1,2,2,cubio_enlarge.shape[3])\n",
    "    step = (1,2,2,cubio_enlarge.shape[3])\n",
    "    patches = view_as_windows(cubio_enlarge, window_shape, step)\n",
    "    patches = patches.squeeze(axis = (3,4))\n",
    "    patches_panel = patches.reshape(-1, patches.shape[-3]*patches.shape[-2]*patches.shape[-1])\n",
    "\n",
    "    dc = patches_panel.mean(axis=1)*2\n",
    "    dc = dc.reshape(n_sample, 32, 32, -1)\n",
    "    cubio_next = np.concatenate((cubio_pca, dc), axis=3)\n",
    "    \n",
    "    return cubio_next\n",
    "\n",
    "def relu(cubio):\n",
    "    cubio_relu = cubio * (cubio > 0)\n",
    "    \n",
    "    return cubio_relu\n",
    "\n",
    "def tanh(cubio):\n",
    "    cubio_tanh = np.divide(np.exp(cubio), 1+np.exp(cubio))\n",
    "    \n",
    "    return cubio_tanh\n",
    "    \n",
    "def complete_base_training(cubio, layer, n_comps, kernel, feature_list, X):\n",
    "    print(\"training at the %dth layer ...\" %(layer))\n",
    "    patches_panel = flatten_patches(cubio,pow(2,layer))\n",
    "    patches_mean_remov = remove_patches_mean(patches_panel)\n",
    "    if layer == 1:\n",
    "        patches_clean = remove_low_variance(patches_mean_remov)\n",
    "    else:\n",
    "        patches_clean = patches_mean_remov\n",
    "    pca_kernel(patches_clean,patches_clean.shape[1], kernel)\n",
    "    cubio_pca = relu(pca_transform_complete(kernel, layer, patches_mean_remov, patches_panel, X))\n",
    "    cubio_next = dc_add(X, layer, cubio, cubio_pca)\n",
    "    #feature_list.append(cubio_next)\n",
    "    print(\"Done! the shape of output cubio is %s.\" %(cubio_next.shape,))\n",
    "    \n",
    "    return cubio_next   \n",
    "    \n",
    "def one_stage_training(layer, n_comps, kernel, feature_list, X):\n",
    "    print(\"training at the %dth layer ...\" %(layer))\n",
    "    patches_panel = flatten_patches(X, pow(2,layer))\n",
    "    patches_mean_remov = remove_patches_mean(patches_panel)\n",
    "    if layer == 1:\n",
    "        patches_clean = remove_low_variance(patches_mean_remov)\n",
    "    else:\n",
    "        patches_clean = patches_mean_remov\n",
    "    pca_kernel(patches_clean, n_comps[layer-1], k)\n",
    "    cubio_next = relu(pca_transform(k, layer, patches_mean_remov, patches_panel, X_train))\n",
    "    feature_list.append(cubio_next)\n",
    "    print(\"Done! the shape of output cubio is %s.\" %(cubio_next.shape,))\n",
    "\n",
    "def feature_fusion(feature_list, num_layers):\n",
    "    feature = feature_list[0].reshape(feature_list[0].shape[0], -1)\n",
    "    for i in range(num_layers-1):\n",
    "        feature = np.concatenate((feature,feature_list[i+1].reshape(feature_list[i+1].shape[0], -1)), axis=1)\n",
    "    print(\"the shape of features we get is %s.\" %(feature.shape,))\n",
    "    return feature\n",
    "\n",
    "def Reduce_Feature(n_comps, feature):\n",
    "    pca = PCA(n_components = n_comps)\n",
    "    X_pc = pca.fit_transform(feature)\n",
    "    print(\"the number of dimensions kept is %d.\" %(X_pc.shape[1]))\n",
    "    \n",
    "    return X_pc, pca\n",
    "\n",
    "def Reduce_Feature_v2(n_comps, feature):\n",
    "    pca = KernelPCA(n_components = n_comps)\n",
    "    X_pc = pca.fit_transform(feature)\n",
    "    print(\"the number of dimensions kept is %d.\" %(X_pc.shape[1]))\n",
    "    \n",
    "    return X_pc, pca\n",
    "\n",
    "def F_test(percent, feature, label):\n",
    "    Ftest = SelectPercentile(chi2, percent)\n",
    "    X_f = Ftest.fit_transform(feature, label)\n",
    "    print(\"the number of feature dimensions passing F-test is %d.\" %(X_f.shape[1]))\n",
    "\n",
    "    return X_f, Ftest\n",
    "\n",
    "def RandomForest_FeatureSelect(percent, feature, label):\n",
    "    forest = RandomForestClassifier(random_state=0, )\n",
    "    forest.fit(feature, label)\n",
    "    index = np.argsort(forest.feature_importances_*100)\n",
    "    num = (int)((float)(feature.shape[1]*percent)/100)\n",
    "    X_rf = feature[:,index[0:num]]\n",
    "    print(\"the number of feature dimensions passing Random-Forest feature selection is %d.\" %(X_rf.shape[1]))\n",
    "    \n",
    "    return X_rf, forest, index\n",
    "\n",
    "def RandomForest_FeatureSelect_test(percent, feature, index):\n",
    "    num = (int)((float)(feature.shape[1]*percent)/100)\n",
    "    X_rf = feature[:,index[0:num]]\n",
    "    print(\"the number of feature dimensions passing Random-Forest feature selection is %d.\" %(X_rf.shape[1]))\n",
    "    \n",
    "    return X_rf\n",
    "    \n",
    "def SVM_training(feature, label, n_comps, percent):\n",
    "    print('SVM is under training...')\n",
    "    X_f, Ftest = F_test(percent, feature, label)\n",
    "    X_pc, pca = Reduce_Feature(n_comps, X_f)\n",
    "    clf = SVC()\n",
    "    clf.fit(X_pc, label)\n",
    "    y_pred = clf.predict(X_pc)\n",
    "    accuracy = accuracy_score(label, y_pred)\n",
    "    print(\"SVM accuracy on training sample is %f\" %(accuracy))\n",
    "    \n",
    "    return Ftest, pca, clf, accuracy\n",
    "\n",
    "def RF_training(feature, label, n_comps, percent):\n",
    "    print('Random Forest is under training...')\n",
    "    X_f, Ftest = F_test(percent, feature, label)\n",
    "    X_pc, pca = Reduce_Feature(n_comps, X_f)\n",
    "    clf = RandomForestClassifier()  \n",
    "    clf.fit(X_pc, label)\n",
    "    y_pred = clf.predict(X_pc)\n",
    "    accuracy = accuracy_score(label, y_pred)\n",
    "    print(\"RF accuracy on training sample is %f\" %(accuracy))\n",
    "    \n",
    "    return Ftest, pca, clf, accuracy\n",
    "\n",
    "def SVM_training_v2(X_pc, label):\n",
    "    print('SVM is under training...')\n",
    "    clf = SVC()\n",
    "    clf.fit(X_pc, label)\n",
    "    y_pred = clf.predict(X_pc)\n",
    "    accuracy = accuracy_score(label, y_pred)\n",
    "    print(\"SVM accuracy on training sample is %f\" %(accuracy))\n",
    "    \n",
    "    return clf, accuracy\n",
    "\n",
    "def one_stage_testing(layer, kernel, feature_list, X):\n",
    "    print(\"training at the %dth layer ...\" %(layer))\n",
    "    patches_panel = flatten_patches(X, pow(2,layer))\n",
    "    patches_mean_remov = remove_patches_mean(patches_panel)\n",
    "    cubio_next = relu(pca_transform(kernel, layer, patches_mean_remov, patches_panel, X))\n",
    "    feature_list.append(cubio_next)\n",
    "    print(\"Done! the shape of output cubio is %s.\" %(cubio_next.shape,))\n",
    "\n",
    "def RF_testing(feature, label, Ftest, pca, clf):\n",
    "    print(\"Test smaples are under Random Forest's testing...\")\n",
    "    feature_test = Ftest.transform(feature)\n",
    "    print(\"the number of feature dimensions passing F-test is %d.\" %(feature_test.shape[1]))\n",
    "    feature_pca = pca.transform(feature_test)\n",
    "    print(\"the number of dimensions kept is %d.\" %(feature_pca.shape[1]))\n",
    "    y_pred = clf.predict(feature_pca)\n",
    "    accuracy = accuracy_score(label, y_pred)     \n",
    "    print(\"RF accuracy on test sample is %f\" %(accuracy))\n",
    "    \n",
    "    return accuracy, y_pred\n",
    "          \n",
    "def SVM_testing(feature, label, Ftest, pca, clf):\n",
    "    print(\"Test smaples are under SVM's testing...\")\n",
    "    feature_test = Ftest.transform(feature)\n",
    "    print(\"the number of feature dimensions passing F-test is %d.\" %(feature_test.shape[1]))\n",
    "    feature_pca = pca.transform(feature_test)\n",
    "    print(\"the number of dimensions kept is %d.\" %(feature_pca.shape[1]))\n",
    "    y_pred = clf.predict(feature_pca)\n",
    "    accuracy = accuracy_score(label, y_pred)     \n",
    "    print(\"SVM accuracy on test sample is %f\" %(accuracy))\n",
    "    \n",
    "    return accuracy, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Saak transform for image reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 32, 32, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare training parameters list\n",
    "k = []\n",
    "feature_list = []\n",
    "n_comps = [3, 4, 7, 6, 8]\n",
    "\n",
    "#preprocess input image batch and label batch\n",
    "X_train, y_train_batch = image_set_preprocessing(X_train_raw, y_train_raw, batch_ratio = 1)\n",
    "cubio = copy.deepcopy(X_train)\n",
    "\n",
    "#compute the number of layers\n",
    "layer_cnt = (int)(log(X_train.shape[1])/log(2))\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training at the 1th layer ...\n",
      "Done! the shape of output cubio is (60000, 16, 16, 7).\n",
      "training at the 2th layer ...\n",
      "Done! the shape of output cubio is (60000, 8, 8, 9).\n",
      "training at the 3th layer ...\n",
      "Done! the shape of output cubio is (60000, 4, 4, 15).\n",
      "training at the 4th layer ...\n",
      "Done! the shape of output cubio is (60000, 2, 2, 13).\n",
      "training at the 5th layer ...\n",
      "Done! the shape of output cubio is (60000, 1, 1, 17).\n"
     ]
    }
   ],
   "source": [
    "for i in range(layer_cnt):\n",
    "    layer = i + 1\n",
    "    one_stage_training(layer, n_comps, k, feature_list, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of features we get is (60000, 2677).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 2677)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#colloect all the features from all layers\n",
    "features = feature_fusion(feature_list, layer_cnt)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.129612"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of feature dimensions passing F-test is 1338.\n",
      "the number of dimensions kept is 32.\n",
      "the number of dimensions kept is 64.\n",
      "the number of dimensions kept is 128.\n",
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.996350\n",
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.995067\n",
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.991517\n"
     ]
    }
   ],
   "source": [
    "#feature selection\n",
    "X_f, Ftest = F_test(50, features, y_train_batch)\n",
    "\n",
    "#feature reduction\n",
    "X_pc_32, pca_32 = Reduce_Feature(32, X_f)\n",
    "X_pc_64, pca_64 = Reduce_Feature(64, X_f)\n",
    "X_pc_128, pca_128 = Reduce_Feature(128, X_f)\n",
    "\n",
    "#SVM training\n",
    "svm_32, accuracy_svm_training_32= SVM_training_v2(X_pc_32, y_train_batch)\n",
    "svm_64, accuracy_svm_training_64= SVM_training_v2(X_pc_64, y_train_batch)\n",
    "svm_128, accuracy_svm_training_128= SVM_training_v2(X_pc_128, y_train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training at the 1th layer ...\n",
      "Done! the shape of output cubio is (10000, 16, 16, 7).\n",
      "training at the 2th layer ...\n",
      "Done! the shape of output cubio is (10000, 8, 8, 9).\n",
      "training at the 3th layer ...\n",
      "Done! the shape of output cubio is (10000, 4, 4, 15).\n",
      "training at the 4th layer ...\n",
      "Done! the shape of output cubio is (10000, 2, 2, 13).\n",
      "training at the 5th layer ...\n",
      "Done! the shape of output cubio is (10000, 1, 1, 17).\n"
     ]
    }
   ],
   "source": [
    "#preprocess testing image batch and label batch \n",
    "X_test, y_test_batch = image_set_preprocessing(X_test_raw, y_test_raw, batch_ratio = 1)\n",
    "\n",
    "#prepare testing parameters list\n",
    "feature_list_test = []\n",
    "\n",
    "#forward testing process\n",
    "for i in range(layer_cnt):\n",
    "    layer = i + 1\n",
    "    one_stage_testing(layer, k, feature_list_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of features we get is (10000, 2677).\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 1338.\n",
      "the number of dimensions kept is 32.\n",
      "SVM accuracy on test sample is 0.982900\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 1338.\n",
      "the number of dimensions kept is 64.\n",
      "SVM accuracy on test sample is 0.985600\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 1338.\n",
      "the number of dimensions kept is 128.\n",
      "SVM accuracy on test sample is 0.983200\n"
     ]
    }
   ],
   "source": [
    "#colloect all the features from all layers\n",
    "features_test = feature_fusion(feature_list_test, layer_cnt)\n",
    "features_test.shape\n",
    "\n",
    "#F-test --> PCA reducing dims --> SVM testing\n",
    "accuracy_svm_testing_32, y_pred_svm_32 = SVM_testing(features_test, y_test_batch, Ftest, pca_32, svm_32)\n",
    "accuracy_svm_testing_64, y_pred_svm_64 = SVM_testing(features_test, y_test_batch, Ftest, pca_64, svm_64)\n",
    "accuracy_svm_testing_128, y_pred_svm_128 = SVM_testing(features_test, y_test_batch, Ftest, pca_128, svm_128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
