{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from math import log\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import MiniBatchSparsePCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "from skimage.transform import resize\n",
    "from skimage.util import *\n",
    "\n",
    "#load MNIST\n",
    "(X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0,
     16,
     25,
     34,
     40,
     46,
     51,
     64,
     81,
     86,
     99,
     114,
     121,
     128,
     135,
     147,
     157,
     169,
     177,
     189
    ]
   },
   "outputs": [],
   "source": [
    "def image_set_preprocessing(X, y, batch_ratio = 1, pad_size=2, pad_method='constant'):\n",
    "    X_pad = np.pad(X, ((0,0),(pad_size,pad_size),(pad_size,pad_size)), 'constant')\n",
    "    \n",
    "    batch_size = (int)(X_pad.shape[0]*batch_ratio)\n",
    "    \n",
    "    order = np.array(range(X_pad.shape[0]))\n",
    "    np.random.shuffle(order)\n",
    "    X_pad_shuffle = X_pad[order]\n",
    "    y_shuffle = y[order]\n",
    "\n",
    "    X_train_batch = ((X_pad_shuffle[0:batch_size, :, :]).astype('float32'))/255\n",
    "    y_train_batch = y_shuffle[0:batch_size,]\n",
    "    X_train_batch = X_train_batch.reshape(X_train_batch.shape[0], X_train_batch.shape[1], X_train_batch.shape[2], 1)\n",
    "    \n",
    "    return X_train_batch, y_train_batch\n",
    "\n",
    "def flatten_patches(cubio, size):\n",
    "    window_shape = (1,size,size,cubio.shape[3])\n",
    "    step = (1,size,size,cubio.shape[3])\n",
    "    patches = view_as_windows(cubio, window_shape, step)\n",
    "    patches = patches.squeeze(axis = (3,4))\n",
    "    patches_panel = patches.reshape(-1, patches.shape[-3]*patches.shape[-2]*patches.shape[-1])\n",
    "    \n",
    "    return patches_panel\n",
    "\n",
    "def flatten_patches_v2(cubio):\n",
    "    window_shape = (1,2,2,cubio.shape[3])\n",
    "    step = (1,2,2,cubio.shape[3])\n",
    "    patches = view_as_windows(cubio, window_shape, step)\n",
    "    patches = patches.squeeze(axis = (3,4))\n",
    "    patches_panel = patches.reshape(-1, patches.shape[-3]*patches.shape[-2]*patches.shape[-1])\n",
    "    \n",
    "    return patches_panel\n",
    "\n",
    "def remove_low_variance(patches_panel,thr=0.05):\n",
    "    std = np.std(patches_panel, axis = 1)\n",
    "    patches_clean = patches_panel[std>thr]\n",
    "    \n",
    "    return patches_clean\n",
    "\n",
    "def remove_patches_mean(patches):\n",
    "    mean = patches.mean(axis = 1)\n",
    "    patches_mean_remov = (patches.T-mean).T\n",
    "    \n",
    "    return patches_mean_remov\n",
    "\n",
    "def pca_kernel(patches, n_comps, kernel):\n",
    "    pca = PCA(n_components = n_comps)\n",
    "    pca.fit(patches)\n",
    "    kernel.append(pca)  \n",
    "    \n",
    "def pca_transform(kernel, layer, patches_mean_remov, patches_panel, X_train):\n",
    "    n_sample = X_train.shape[0]\n",
    "    h = (int)(X_train.shape[1]/pow(2,layer))\n",
    "    patches_proj = k[layer-1].transform(patches_mean_remov)\n",
    "    cubio_pos = patches_proj.reshape(n_sample,h,h,-1)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=3)\n",
    "    dc = patches_panel.mean(axis=1)*2\n",
    "    dc = dc.reshape(n_sample, h, h, -1)\n",
    "    cubio_next = np.concatenate((cubio_pca, dc), axis=3)\n",
    "    \n",
    "    return cubio_next\n",
    "\n",
    "def dc_add(X_train, layer, cubio, cubio_pca):\n",
    "    n_sample = 60000\n",
    "    cubio_enlarge = np.zeros((n_sample,32*pow(2,1),32*pow(2,1),cubio.shape[3]))\n",
    "    for i in range(n_sample):\n",
    "        cubio_enlarge[i,:,:,:] = resize(cubio[i,:,:,:], output_shape=[cubio.shape[1]*2,cubio.shape[2]*2,cubio.shape[3]])\n",
    "    window_shape = (1,2,2,cubio_enlarge.shape[3])\n",
    "    step = (1,2,2,cubio_enlarge.shape[3])\n",
    "    patches = view_as_windows(cubio_enlarge, window_shape, step)\n",
    "    patches = patches.squeeze(axis = (3,4))\n",
    "    patches_panel = patches.reshape(-1, patches.shape[-3]*patches.shape[-2]*patches.shape[-1])\n",
    "\n",
    "    dc = patches_panel.mean(axis=1)*2\n",
    "    dc = dc.reshape(n_sample, 32, 32, -1)\n",
    "    cubio_next = np.concatenate((cubio_pca, dc), axis=3)\n",
    "    \n",
    "    return cubio_next\n",
    "\n",
    "def relu(cubio):\n",
    "    cubio_relu = cubio * (cubio > 0)\n",
    "    \n",
    "    return cubio_relu\n",
    "     \n",
    "def one_stage_training(layer, n_comps, kernel, feature_list, X):\n",
    "    print(\"training at the %dth layer ...\" %(layer))\n",
    "    patches_panel = flatten_patches(X, pow(2,layer))\n",
    "    patches_mean_remov = remove_patches_mean(patches_panel)\n",
    "    if layer == 1:\n",
    "        patches_clean = remove_low_variance(patches_mean_remov)\n",
    "    else:\n",
    "        patches_clean = patches_mean_remov\n",
    "    pca_kernel(patches_clean, n_comps[layer-1], k)\n",
    "    cubio_next = relu(pca_transform(k, layer, patches_mean_remov, patches_panel, X_train))\n",
    "    feature_list.append(cubio_next)\n",
    "    print(\"Done! the shape of output cubio is %s.\" %(cubio_next.shape,))\n",
    "    \n",
    "def one_stage_training_v2(cubio, layer, n_comps, kernel, feature_list, X):\n",
    "    print(\"training at the %dth layer ...\" %(layer))\n",
    "    patches_panel = flatten_patches_v2(cubio)\n",
    "    patches_mean_remov = remove_patches_mean(patches_panel)\n",
    "    if layer == 1:\n",
    "        patches_clean = remove_low_variance(patches_mean_remov)\n",
    "    else:\n",
    "        patches_clean = patches_mean_remov\n",
    "    pca_kernel(patches_clean,n_comps[layer-1], kernel)\n",
    "    cubio_next = relu(pca_transform(kernel, layer, patches_mean_remov, patches_panel, X))\n",
    "    feature_list.append(cubio_next)\n",
    "    print(\"Done! the shape of output cubio is %s.\" %(cubio_next.shape,))\n",
    "    \n",
    "    return cubio_next\n",
    "\n",
    "def feature_fusion(feature_list, num_layers):\n",
    "    feature = feature_list[0].reshape(feature_list[0].shape[0], -1)\n",
    "    for i in range(num_layers-1):\n",
    "        feature = np.concatenate((feature,feature_list[i+1].reshape(feature_list[i+1].shape[0], -1)), axis=1)\n",
    "    print(\"the shape of features we get is %s.\" %(feature.shape,))\n",
    "    return feature\n",
    "\n",
    "def Reduce_Feature(n_comps, feature):\n",
    "    pca = PCA(n_components = n_comps)\n",
    "    X_pc = pca.fit_transform(feature)\n",
    "    print(\"the number of dimensions kept is %d.\" %(X_pc.shape[1]))\n",
    "    \n",
    "    return X_pc, pca\n",
    "\n",
    "def F_test(percent, feature, label):\n",
    "    Ftest = SelectPercentile(chi2, percent)\n",
    "    X_f = Ftest.fit_transform(feature, label)\n",
    "    print(\"the number of feature dimensions passing F-test is %d.\" %(X_f.shape[1]))\n",
    "\n",
    "    return X_f, Ftest\n",
    "    \n",
    "def SVM_training(feature, label, n_comps, percent):\n",
    "    print('SVM is under training...')\n",
    "    X_f, Ftest = F_test(percent, feature, label)\n",
    "    X_pc, pca = Reduce_Feature(n_comps, X_f)\n",
    "    clf = SVC()\n",
    "    clf.fit(X_pc, label)\n",
    "    y_pred = clf.predict(X_pc)\n",
    "    accuracy = accuracy_score(label, y_pred)\n",
    "    print(\"SVM accuracy on training sample is %f\" %(accuracy))\n",
    "    \n",
    "    return Ftest, pca, clf, accuracy\n",
    "\n",
    "def SVM_training_v2(X_pc, label):\n",
    "    print('SVM is under training...')\n",
    "    clf = SVC()\n",
    "    clf.fit(X_pc, label)\n",
    "    y_pred = clf.predict(X_pc)\n",
    "    accuracy = accuracy_score(label, y_pred)\n",
    "    print(\"SVM accuracy on training sample is %f\" %(accuracy))\n",
    "    \n",
    "    return clf, accuracy\n",
    "\n",
    "def RF_training(feature, label, n_comps, percent):\n",
    "    print('Random Forest is under training...')\n",
    "    X_f, Ftest = F_test(percent, feature, label)\n",
    "    X_pc, pca = Reduce_Feature(n_comps, X_f)\n",
    "    clf = RandomForestClassifier()  \n",
    "    clf.fit(X_pc, label)\n",
    "    y_pred = clf.predict(X_pc)\n",
    "    accuracy = accuracy_score(label, y_pred)\n",
    "    print(\"RF accuracy on training sample is %f\" %(accuracy))\n",
    "    \n",
    "    return Ftest, pca, clf, accuracy\n",
    "\n",
    "def one_stage_testing(layer, kernel, feature_list, X):\n",
    "    print(\"training at the %dth layer ...\" %(layer))\n",
    "    patches_panel = flatten_patches(X, pow(2,layer))\n",
    "    patches_mean_remov = remove_patches_mean(patches_panel)\n",
    "    cubio_next = relu(pca_transform(kernel, layer, patches_mean_remov, patches_panel, X))\n",
    "    feature_list.append(cubio_next)\n",
    "    print(\"Done! the shape of output cubio is %s.\" %(cubio_next.shape,))\n",
    "\n",
    "def RF_testing(feature, label, Ftest, pca, clf):\n",
    "    print(\"Test smaples are under Random Forest's testing...\")\n",
    "    feature_test = Ftest.transform(feature)\n",
    "    print(\"the number of feature dimensions passing F-test is %d.\" %(feature_test.shape[1]))\n",
    "    feature_pca = pca.transform(feature_test)\n",
    "    print(\"the number of dimensions kept is %d.\" %(feature_pca.shape[1]))\n",
    "    y_pred = clf.predict(feature_pca)\n",
    "    accuracy = accuracy_score(label, y_pred)     \n",
    "    print(\"RF accuracy on test sample is %f\" %(accuracy))\n",
    "    \n",
    "    return accuracy, y_pred\n",
    "          \n",
    "def SVM_testing(feature, label, Ftest, pca, clf):\n",
    "    print(\"Test smaples are under SVM's testing...\")\n",
    "    feature_test = Ftest.transform(feature)\n",
    "    print(\"the number of feature dimensions passing F-test is %d.\" %(feature_test.shape[1]))\n",
    "    feature_pca = pca.transform(feature_test)\n",
    "    print(\"the number of dimensions kept is %d.\" %(feature_pca.shape[1]))\n",
    "    y_pred = clf.predict(feature_pca)\n",
    "    accuracy = accuracy_score(label, y_pred)     \n",
    "    print(\"SVM accuracy on test sample is %f\" %(accuracy))\n",
    "    \n",
    "    return accuracy, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the first version -- parallel structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#prepare training parameters list\n",
    "k = []\n",
    "k_means = []\n",
    "\n",
    "feature_list = []\n",
    "n_comps = [3, 4, 7, 6, 8]\n",
    "\n",
    "#preprocess input image batch and label batch\n",
    "X_train, y_train_batch = image_set_preprocessing(X_train_raw, y_train_raw, batch_ratio = 1)\n",
    "cubio = copy.deepcopy(X_train)\n",
    "\n",
    "#compute the number of layers\n",
    "layer_cnt = (int)(log(X_train.shape[1])/log(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the first stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubio_1 = flatten_patches(cubio, 2)\n",
    "\n",
    "cubio_1_remov = remove_patches_mean(cubio_1)\n",
    "\n",
    "cubio_1_clean = remove_low_variance(cubio_1_remov,thr=0.05)\n",
    "\n",
    "PCA_s = PCA(n_components=3).fit(cubio_1_clean)\n",
    "k.append(PCA_s)\n",
    "n_sample = X_train.shape[0]\n",
    "h = 16\n",
    "patches_proj = k[0].transform(cubio_1_remov)\n",
    "cubio_pos = patches_proj.reshape(n_sample,h,h,-1)\n",
    "cubio_neg = -cubio_pos\n",
    "cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=3)\n",
    "dc = cubio_1.mean(axis=1)*2\n",
    "dc = dc.reshape(n_sample, h, h, -1)\n",
    "cubio_next = relu(np.concatenate((cubio_pca, dc), axis=3))\n",
    "feature_list.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the second stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubio_2 = flatten_patches(cubio, 4)\n",
    "\n",
    "cubio_2_remov = remove_patches_mean(cubio_2)\n",
    "\n",
    "PCA_s = PCA(n_components=4)\n",
    "k.append(PCA_s)\n",
    "\n",
    "h = 8\n",
    "patches_proj = k[1].fit_transform(cubio_2_remov)\n",
    "cubio_pos = patches_proj.reshape(n_sample,h,h,-1)\n",
    "cubio_neg = -cubio_pos\n",
    "cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=3)\n",
    "dc = cubio_2.mean(axis=1)*2\n",
    "dc = dc.reshape(n_sample, h, h, -1)\n",
    "cubio_next = relu(np.concatenate((cubio_pca, dc), axis=3))\n",
    "feature_list.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the third stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960000, 64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cubio_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   0 (elapsed time:  19s,  0.3mn)\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n",
      "Adding new random atom\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-e31c55b98315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpatches_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcubio_3_remov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mcubio_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatches_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcubio_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcubio_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/decomposition/sparse_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             return_n_iter=True)\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/decomposition/dict_learning.py\u001b[0m in \u001b[0;36mdict_learning_online\u001b[0;34m(X, n_components, alpha, n_iter, return_code, dict_init, callback, batch_size, verbose, shuffle, n_jobs, method, iter_offset, random_state, return_inner_stats, inner_stats, return_n_iter)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;31m# Update dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         dictionary = _update_dict(dictionary, B, A, verbose=verbose,\n\u001b[0;32m--> 757\u001b[0;31m                                   random_state=random_state)\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;31m# XXX: Can the residuals be of any use?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/decomposition/dict_learning.py\u001b[0m in \u001b[0;36m_update_dict\u001b[0;34m(dictionary, Y, code, verbose, return_r2, random_state)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Adding new random atom\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0;31m# Setting corresponding coefs to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mcode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cubio_3 = flatten_patches(cubio, 8)\n",
    "\n",
    "cubio_3_remov = remove_patches_mean(cubio_3)\n",
    "\n",
    "PCA_s = MiniBatchSparsePCA(n_components=100, alpha=0.8, n_iter=20, batch_size=30, verbose=2)\n",
    "k.append(PCA_s)\n",
    "\n",
    "h = 4\n",
    "patches_proj = k[2].fit_transform(cubio_3_remov)\n",
    "cubio_pos = patches_proj.reshape(n_sample,h,h,-1)\n",
    "cubio_neg = -cubio_pos\n",
    "cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=3)\n",
    "dc = cubio_3.mean(axis=1)*2\n",
    "dc = dc.reshape(n_sample, h, h, -1)\n",
    "cubio_next = relu(np.concatenate((cubio_pca, dc), axis=3))\n",
    "feature_list.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the fourth stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubio_4 = flatten_patches(cubio, 16)\n",
    "\n",
    "cubio_4_remov = remove_patches_mean(cubio_4)\n",
    "\n",
    "cubio_4_clean = cubio_4_remov\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=3, max_iter=50, verbose=0, random_state=0)\n",
    "kmeans.fit(cubio_4_clean)\n",
    "k_means.append(kmeans)\n",
    "\n",
    "\n",
    "PCA_idx = kmeans.predict(cubio_4_remov)\n",
    "\n",
    "PCA_obj = []\n",
    "cubio_next = np.zeros((PCA_idx.shape[0], 37))\n",
    "k.append(PCA_obj)\n",
    "for cls in range(3):\n",
    "    ind = PCA_idx==cls\n",
    "    cubio_cls = cubio_4_remov[ind,:]\n",
    "    PCA_obj.append(PCA(n_components = 6).fit(cubio_cls))\n",
    "    n_sample = X_train.shape[0]\n",
    "    h = 2\n",
    "    cubio_pos = PCA_obj[cls].transform(cubio_cls)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=1)\n",
    "    cubio_next[ind,cls*12:(cls+1)*12] = cubio_pca\n",
    "    dc = cubio_4[ind,:].mean(axis=1)*2\n",
    "    cubio_next[ind,36] = dc\n",
    "\n",
    "cubio_next = relu(cubio_next.reshape(X_train.shape[0], 2, 2, 37))\n",
    "feature_list.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the fifth stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubio_5 = flatten_patches(cubio, 32)\n",
    "\n",
    "cubio_5_remov = remove_patches_mean(cubio_5)\n",
    "\n",
    "cubio_5_clean = cubio_5_remov\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=3, max_iter=50, verbose=0, random_state=0)\n",
    "kmeans.fit(cubio_5_clean)\n",
    "k_means.append(kmeans)\n",
    "\n",
    "\n",
    "PCA_idx = kmeans.predict(cubio_5_remov)\n",
    "\n",
    "PCA_obj = []\n",
    "cubio_next = np.zeros((PCA_idx.shape[0], 49))\n",
    "k.append(PCA_obj)\n",
    "for cls in range(3):\n",
    "    ind = PCA_idx==cls\n",
    "    cubio_cls = cubio_5_remov[ind,:]\n",
    "    PCA_obj.append(PCA(n_components = 8).fit(cubio_cls))\n",
    "    n_sample = X_train.shape[0]\n",
    "    h = 1\n",
    "    cubio_pos = PCA_obj[cls].transform(cubio_cls)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=1)\n",
    "    cubio_next[ind,cls*16:(cls+1)*16] = cubio_pca\n",
    "    dc = cubio_5[ind,:].mean(axis=1)*2\n",
    "    cubio_next[ind,48] = dc\n",
    "\n",
    "cubio_next = relu(cubio_next.reshape(X_train.shape[0], 1, 1, 49))\n",
    "feature_list.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### fuse the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of features we get is (60000, 3253).\n"
     ]
    }
   ],
   "source": [
    "features = feature_fusion(feature_list, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### machine learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of feature dimensions passing F-test is 1626.\n",
      "the number of dimensions kept is 32.\n",
      "the number of dimensions kept is 64.\n",
      "the number of dimensions kept is 128.\n",
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.995267\n",
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.994050\n",
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.990600\n"
     ]
    }
   ],
   "source": [
    "#feature selection\n",
    "X_f, Ftest = F_test(50, features, y_train_batch)\n",
    "\n",
    "#feature reduction\n",
    "X_pc_32, pca_32 = Reduce_Feature(32, X_f)\n",
    "X_pc_64, pca_64 = Reduce_Feature(64, X_f)\n",
    "X_pc_128, pca_128 = Reduce_Feature(128, X_f)\n",
    "\n",
    "#SVM training\n",
    "svm_32, accuracy_svm_training_32= SVM_training_v2(X_pc_32, y_train_batch)\n",
    "svm_64, accuracy_svm_training_64= SVM_training_v2(X_pc_64, y_train_batch)\n",
    "svm_128, accuracy_svm_training_128= SVM_training_v2(X_pc_128, y_train_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#preprocess testing image batch and label batch \n",
    "X_test, y_test_batch = image_set_preprocessing(X_test_raw, y_test_raw, batch_ratio = 1)\n",
    "\n",
    "#prepare testing parameters list\n",
    "feature_list_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### the first stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cubio_1_test = flatten_patches(X_test, 2)\n",
    "\n",
    "cubio_1_remov_test = remove_patches_mean(cubio_1_test)\n",
    "\n",
    "n_sample = X_test.shape[0]\n",
    "h = 16\n",
    "patches_proj = k[0][0].transform(cubio_1_remov_test)\n",
    "cubio_pos = patches_proj.reshape(n_sample,h,h,-1)\n",
    "cubio_neg = -cubio_pos\n",
    "cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=3)\n",
    "dc = cubio_1_test.mean(axis=1)*2\n",
    "dc = dc.reshape(n_sample, h, h, -1)\n",
    "cubio_next = relu(np.concatenate((cubio_pca, dc), axis=3))\n",
    "feature_list_test.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### the second stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cubio_2_test = flatten_patches(X_test, 4)\n",
    "\n",
    "cubio_2_remov_test = remove_patches_mean(cubio_2_test)\n",
    "\n",
    "h = 8\n",
    "patches_proj = k[1][0].transform(cubio_2_remov_test)\n",
    "cubio_pos = patches_proj.reshape(n_sample,h,h,-1)\n",
    "cubio_neg = -cubio_pos\n",
    "cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=3)\n",
    "dc = cubio_2_test.mean(axis=1)*2\n",
    "dc = dc.reshape(n_sample, h, h, -1)\n",
    "cubio_next = relu(np.concatenate((cubio_pca, dc), axis=3))\n",
    "feature_list_test.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### the third stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cubio_3_test = flatten_patches(X_test, 8)\n",
    "\n",
    "cubio_3_remov_test = remove_patches_mean(cubio_3_test)\n",
    "\n",
    "PCA_idx = k_means[0].predict(cubio_3_remov_test)\n",
    "\n",
    "cubio_next = np.zeros((PCA_idx.shape[0], 43))\n",
    "\n",
    "for cls in range(3):\n",
    "    ind = PCA_idx==cls\n",
    "    cubio_cls_test = cubio_3_remov_test[ind,:]\n",
    "    h = 4\n",
    "    cubio_pos = k[2][cls].transform(cubio_cls_test)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=1)\n",
    "    cubio_next[ind,cls*14:(cls+1)*14] = cubio_pca\n",
    "    dc = cubio_3_test[ind,:].mean(axis=1)*2\n",
    "    cubio_next[ind,42] = dc\n",
    "\n",
    "cubio_next = relu(cubio_next.reshape(n_sample, 4, 4, 43))\n",
    "feature_list_test.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### the fourth stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cubio_4_test = flatten_patches(X_test, 16)\n",
    "\n",
    "cubio_4_remov_test = remove_patches_mean(cubio_4_test)\n",
    "\n",
    "PCA_idx = k_means[1].predict(cubio_4_remov_test)\n",
    "\n",
    "cubio_next = np.zeros((PCA_idx.shape[0], 37))\n",
    "\n",
    "for cls in range(3):\n",
    "    ind = PCA_idx==cls\n",
    "    cubio_cls = cubio_4_remov_test[ind,:]\n",
    "    h = 2\n",
    "    cubio_pos = k[3][cls].transform(cubio_cls)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=1)\n",
    "    cubio_next[ind,cls*12:(cls+1)*12] = cubio_pca\n",
    "    dc = cubio_4_test[ind,:].mean(axis=1)*2\n",
    "    cubio_next[ind,36] = dc\n",
    "\n",
    "cubio_next = relu(cubio_next.reshape(n_sample, 2, 2, 37))\n",
    "feature_list_test.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the fifth stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubio_5_test = flatten_patches(X_test, 32)\n",
    "\n",
    "cubio_5_remov_test = remove_patches_mean(cubio_5_test)\n",
    "\n",
    "PCA_idx = k_means[2].predict(cubio_5_remov_test)\n",
    "\n",
    "cubio_next = np.zeros((PCA_idx.shape[0], 49))\n",
    "\n",
    "for cls in range(3):\n",
    "    ind = PCA_idx==cls\n",
    "    cubio_cls = cubio_5_remov_test[ind,:]\n",
    "    h = 1\n",
    "    cubio_pos = k[4][cls].transform(cubio_cls)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=1)\n",
    "    cubio_next[ind,cls*16:(cls+1)*16] = cubio_pca\n",
    "    dc = cubio_5_test[ind,:].mean(axis=1)*2\n",
    "    cubio_next[ind,48] = dc\n",
    "\n",
    "cubio_next = relu(cubio_next.reshape(n_sample, 1, 1, 49))\n",
    "feature_list_test.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fuse  the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of features we get is (10000, 3253).\n"
     ]
    }
   ],
   "source": [
    "features_test = feature_fusion(feature_list_test, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 1626.\n",
      "the number of dimensions kept is 32.\n",
      "SVM accuracy on test sample is 0.979900\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 1626.\n",
      "the number of dimensions kept is 64.\n",
      "SVM accuracy on test sample is 0.981800\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 1626.\n",
      "the number of dimensions kept is 128.\n",
      "SVM accuracy on test sample is 0.981300\n"
     ]
    }
   ],
   "source": [
    "#F-test --> PCA reducing dims --> SVM testing\n",
    "accuracy_svm_testing_32, y_pred_svm_32 = SVM_testing(features_test, y_test_batch, Ftest, pca_32, svm_32)\n",
    "accuracy_svm_testing_64, y_pred_svm_64 = SVM_testing(features_test, y_test_batch, Ftest, pca_64, svm_64)\n",
    "accuracy_svm_testing_128, y_pred_svm_128 = SVM_testing(features_test, y_test_batch, Ftest, pca_128, svm_128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The second version -- tree structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#prepare training parameters list\n",
    "k = []\n",
    "k_means = []\n",
    "\n",
    "feature_list = []\n",
    "n_comps = [3, 4, 7, 6, 8]\n",
    "\n",
    "#preprocess input image batch and label batch\n",
    "X_train, y_train_batch = image_set_preprocessing(X_train_raw, y_train_raw, batch_ratio = 1)\n",
    "cubio = copy.deepcopy(X_train)\n",
    "\n",
    "#compute the number of layers\n",
    "layer_cnt = (int)(log(X_train.shape[1])/log(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### the first stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cubio_1 = flatten_patches_v2(cubio)\n",
    "\n",
    "cubio_1_remov = remove_patches_mean(cubio_1)\n",
    "\n",
    "cubio_1_clean = remove_low_variance(cubio_1_remov,thr=0.05)\n",
    "\n",
    "PCA_obj = []\n",
    "k.append(PCA_obj)\n",
    "\n",
    "PCA_obj.append(PCA(n_components = 3).fit(cubio_1_clean))\n",
    "n_sample = X_train.shape[0]\n",
    "h = 16\n",
    "patches_proj = PCA_obj[0].transform(cubio_1_remov)\n",
    "cubio_pos = patches_proj.reshape(n_sample,h,h,-1)\n",
    "cubio_neg = -cubio_pos\n",
    "cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=3)\n",
    "dc = cubio_1.mean(axis=1)*2\n",
    "dc = dc.reshape(n_sample, h, h, -1)\n",
    "cubio_next = relu(np.concatenate((cubio_pca, dc), axis=3))\n",
    "feature_list.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### the second stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cubio_2 = flatten_patches_v2(cubio_next)\n",
    "\n",
    "cubio_2_remov = remove_patches_mean(cubio_2)\n",
    "\n",
    "cubio_2_clean = cubio_2_remov\n",
    "\n",
    "PCA_obj = []\n",
    "k.append(PCA_obj)\n",
    "\n",
    "PCA_obj.append(PCA(n_components = 4).fit(cubio_2_clean))\n",
    "h = 8\n",
    "patches_proj = PCA_obj[0].transform(cubio_2_remov)\n",
    "cubio_pos = patches_proj.reshape(n_sample,h,h,-1)\n",
    "cubio_neg = -cubio_pos\n",
    "cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=3)\n",
    "dc = cubio_2.mean(axis=1)*2\n",
    "dc = dc.reshape(n_sample, h, h, -1)\n",
    "cubio_next = relu(np.concatenate((cubio_pca, dc), axis=3))\n",
    "feature_list.append(cubio_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 8, 8, 9)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cubio_next.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### the third stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cubio_3 = flatten_patches_v2(cubio_next)\n",
    "\n",
    "cubio_3_remov = remove_patches_mean(cubio_3)\n",
    "\n",
    "cubio_3_clean = cubio_3_remov\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=3, max_iter=50, verbose=0, random_state=0)\n",
    "kmeans.fit(cubio_3_clean)\n",
    "k_means.append(kmeans)\n",
    "\n",
    "PCA_idx = kmeans.predict(cubio_3_remov)\n",
    "\n",
    "PCA_obj = []\n",
    "cubio_next = np.zeros((PCA_idx.shape[0], 43))\n",
    "k.append(PCA_obj)\n",
    "for cls in range(3):\n",
    "    ind = PCA_idx==cls\n",
    "    cubio_cls = cubio_3_remov[ind,:]\n",
    "    PCA_obj.append(PCA(n_components = 7).fit(cubio_cls))\n",
    "    n_sample = X_train.shape[0]\n",
    "    h = 4\n",
    "    cubio_pos = PCA_obj[cls].transform(cubio_cls)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=1)\n",
    "    cubio_next[ind,cls*14:(cls+1)*14] = cubio_pca\n",
    "    dc = cubio_3[ind,:].mean(axis=1)*2\n",
    "    cubio_next[ind,42] = dc\n",
    "\n",
    "cubio_next = relu(cubio_next.reshape(X_train.shape[0], 4, 4, 43))\n",
    "feature_list.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### the fourth stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cubio_4 = flatten_patches_v2(cubio_next)\n",
    "\n",
    "cubio_4_remov = remove_patches_mean(cubio_4)\n",
    "\n",
    "cubio_4_clean = cubio_4_remov\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=3, max_iter=50, verbose=0, random_state=0)\n",
    "kmeans.fit(cubio_4_clean)\n",
    "k_means.append(kmeans)\n",
    "\n",
    "\n",
    "PCA_idx = kmeans.predict(cubio_4_remov)\n",
    "\n",
    "PCA_obj = []\n",
    "cubio_next = np.zeros((PCA_idx.shape[0], 37))\n",
    "k.append(PCA_obj)\n",
    "for cls in range(3):\n",
    "    ind = PCA_idx==cls\n",
    "    cubio_cls = cubio_4_remov[ind,:]\n",
    "    PCA_obj.append(PCA(n_components = 6).fit(cubio_cls))\n",
    "    n_sample = X_train.shape[0]\n",
    "    h = 2\n",
    "    cubio_pos = PCA_obj[cls].transform(cubio_cls)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=1)\n",
    "    cubio_next[ind,cls*12:(cls+1)*12] = cubio_pca\n",
    "    dc = cubio_4[ind,:].mean(axis=1)*2\n",
    "    cubio_next[ind,36] = dc\n",
    "\n",
    "cubio_next = relu(cubio_next.reshape(X_train.shape[0], 2, 2, 37))\n",
    "feature_list.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### the fifth stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cubio_5 = flatten_patches_v2(cubio_next)\n",
    "\n",
    "cubio_5_remov = remove_patches_mean(cubio_5)\n",
    "\n",
    "cubio_5_clean = cubio_5_remov\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=3, max_iter=50, verbose=0, random_state=0)\n",
    "kmeans.fit(cubio_5_clean)\n",
    "k_means.append(kmeans)\n",
    "\n",
    "\n",
    "PCA_idx = kmeans.predict(cubio_5_remov)\n",
    "\n",
    "PCA_obj = []\n",
    "cubio_next = np.zeros((PCA_idx.shape[0], 49))\n",
    "k.append(PCA_obj)\n",
    "for cls in range(3):\n",
    "    ind = PCA_idx==cls\n",
    "    cubio_cls = cubio_5_remov[ind,:]\n",
    "    PCA_obj.append(PCA(n_components = 8).fit(cubio_cls))\n",
    "    n_sample = X_train.shape[0]\n",
    "    h = 1\n",
    "    cubio_pos = PCA_obj[cls].transform(cubio_cls)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=1)\n",
    "    cubio_next[ind,cls*16:(cls+1)*16] = cubio_pca\n",
    "    dc = cubio_5[ind,:].mean(axis=1)*2\n",
    "    cubio_next[ind,48] = dc\n",
    "\n",
    "cubio_next = relu(cubio_next.reshape(X_train.shape[0], 1, 1, 49))\n",
    "feature_list.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### fuse the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of features we get is (60000, 3253).\n"
     ]
    }
   ],
   "source": [
    "features = feature_fusion(feature_list, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### machine learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of feature dimensions passing F-test is 1626.\n",
      "the number of dimensions kept is 32.\n",
      "the number of dimensions kept is 64.\n",
      "the number of dimensions kept is 128.\n",
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.994650\n",
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.993083\n",
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.988700\n"
     ]
    }
   ],
   "source": [
    "#feature selection\n",
    "X_f, Ftest = F_test(50, features, y_train_batch)\n",
    "\n",
    "#feature reduction\n",
    "X_pc_32, pca_32 = Reduce_Feature(32, X_f)\n",
    "X_pc_64, pca_64 = Reduce_Feature(64, X_f)\n",
    "X_pc_128, pca_128 = Reduce_Feature(128, X_f)\n",
    "\n",
    "#SVM training\n",
    "svm_32, accuracy_svm_training_32= SVM_training_v2(X_pc_32, y_train_batch)\n",
    "svm_64, accuracy_svm_training_64= SVM_training_v2(X_pc_64, y_train_batch)\n",
    "svm_128, accuracy_svm_training_128= SVM_training_v2(X_pc_128, y_train_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#preprocess testing image batch and label batch \n",
    "X_test, y_test_batch = image_set_preprocessing(X_test_raw, y_test_raw, batch_ratio = 1)\n",
    "cubio = copy.deepcopy(X_test)\n",
    "\n",
    "#prepare testing parameters list\n",
    "feature_list_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### the first stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cubio_1_test = flatten_patches_v2(cubio)\n",
    "\n",
    "cubio_1_remov_test = remove_patches_mean(cubio_1_test)\n",
    "\n",
    "n_sample = X_test.shape[0]\n",
    "h = 16\n",
    "patches_proj = k[0][0].transform(cubio_1_remov_test)\n",
    "cubio_pos = patches_proj.reshape(n_sample,h,h,-1)\n",
    "cubio_neg = -cubio_pos\n",
    "cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=3)\n",
    "dc = cubio_1_test.mean(axis=1)*2\n",
    "dc = dc.reshape(n_sample, h, h, -1)\n",
    "cubio_next = relu(np.concatenate((cubio_pca, dc), axis=3))\n",
    "feature_list_test.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### the second stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cubio_2_test = flatten_patches_v2(cubio_next)\n",
    "\n",
    "cubio_2_remov_test = remove_patches_mean(cubio_2_test)\n",
    "\n",
    "h = 8\n",
    "patches_proj = k[1][0].transform(cubio_2_remov_test)\n",
    "cubio_pos = patches_proj.reshape(n_sample,h,h,-1)\n",
    "cubio_neg = -cubio_pos\n",
    "cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=3)\n",
    "dc = cubio_2_test.mean(axis=1)*2\n",
    "dc = dc.reshape(n_sample, h, h, -1)\n",
    "cubio_next = relu(np.concatenate((cubio_pca, dc), axis=3))\n",
    "feature_list_test.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### the third stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cubio_3_test = flatten_patches_v2(cubio_next)\n",
    "\n",
    "cubio_3_remov_test = remove_patches_mean(cubio_3_test)\n",
    "\n",
    "PCA_idx = k_means[0].predict(cubio_3_remov_test)\n",
    "\n",
    "cubio_next = np.zeros((PCA_idx.shape[0], 43))\n",
    "\n",
    "for cls in range(3):\n",
    "    ind = PCA_idx==cls\n",
    "    cubio_cls_test = cubio_3_remov_test[ind,:]\n",
    "    h = 4\n",
    "    cubio_pos = k[2][cls].transform(cubio_cls_test)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=1)\n",
    "    cubio_next[ind,cls*14:(cls+1)*14] = cubio_pca\n",
    "    dc = cubio_3_test[ind,:].mean(axis=1)*2\n",
    "    cubio_next[ind,42] = dc\n",
    "\n",
    "cubio_next = relu(cubio_next.reshape(n_sample, 4, 4, 43))\n",
    "feature_list_test.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### the fourth stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cubio_4_test = flatten_patches_v2(cubio_next)\n",
    "\n",
    "cubio_4_remov_test = remove_patches_mean(cubio_4_test)\n",
    "\n",
    "PCA_idx = k_means[1].predict(cubio_4_remov_test)\n",
    "\n",
    "cubio_next = np.zeros((PCA_idx.shape[0], 37))\n",
    "\n",
    "for cls in range(3):\n",
    "    ind = PCA_idx==cls\n",
    "    cubio_cls = cubio_4_remov_test[ind,:]\n",
    "    h = 2\n",
    "    cubio_pos = k[3][cls].transform(cubio_cls)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=1)\n",
    "    cubio_next[ind,cls*12:(cls+1)*12] = cubio_pca\n",
    "    dc = cubio_4_test[ind,:].mean(axis=1)*2\n",
    "    cubio_next[ind,36] = dc\n",
    "\n",
    "cubio_next = relu(cubio_next.reshape(n_sample, 2, 2, 37))\n",
    "feature_list_test.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the fifth stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubio_5_test = flatten_patches_v2(cubio_next)\n",
    "\n",
    "cubio_5_remov_test = remove_patches_mean(cubio_5_test)\n",
    "\n",
    "PCA_idx = k_means[2].predict(cubio_5_remov_test)\n",
    "\n",
    "cubio_next = np.zeros((PCA_idx.shape[0], 49))\n",
    "\n",
    "for cls in range(3):\n",
    "    ind = PCA_idx==cls\n",
    "    cubio_cls = cubio_5_remov_test[ind,:]\n",
    "    h = 1\n",
    "    cubio_pos = k[4][cls].transform(cubio_cls)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=1)\n",
    "    cubio_next[ind,cls*16:(cls+1)*16] = cubio_pca\n",
    "    dc = cubio_5_test[ind,:].mean(axis=1)*2\n",
    "    cubio_next[ind,48] = dc\n",
    "\n",
    "cubio_next = relu(cubio_next.reshape(n_sample, 1, 1, 49))\n",
    "feature_list_test.append(cubio_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fuse the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of features we get is (10000, 3253).\n"
     ]
    }
   ],
   "source": [
    "features_test = feature_fusion(feature_list_test, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 1626.\n",
      "the number of dimensions kept is 32.\n",
      "SVM accuracy on test sample is 0.980800\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 1626.\n",
      "the number of dimensions kept is 64.\n",
      "SVM accuracy on test sample is 0.981600\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 1626.\n",
      "the number of dimensions kept is 128.\n",
      "SVM accuracy on test sample is 0.978900\n"
     ]
    }
   ],
   "source": [
    "#F-test --> PCA reducing dims --> SVM testing\n",
    "accuracy_svm_testing_32, y_pred_svm_32 = SVM_testing(features_test, y_test_batch, Ftest, pca_32, svm_32)\n",
    "accuracy_svm_testing_64, y_pred_svm_64 = SVM_testing(features_test, y_test_batch, Ftest, pca_64, svm_64)\n",
    "accuracy_svm_testing_128, y_pred_svm_128 = SVM_testing(features_test, y_test_batch, Ftest, pca_128, svm_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from time import time\n",
    "\n",
    "from numpy.random import RandomState\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import decomposition\n",
    "\n",
    "image_shape = (64, 64)\n",
    "rng = RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to /Users/shanlinsun/scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "# Load faces data\n",
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=rng)\n",
    "faces = dataset.data\n",
    "\n",
    "n_samples, n_features = faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global centering\n",
    "faces_centered = faces - faces.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local centering\n",
    "faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces_centered.mean(axis=1).reshape(n_samples, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = decomposition.MiniBatchSparsePCA(n_components=6, alpha=0.8,\n",
    "                                      n_iter=100, batch_size=3,\n",
    "                                      random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchSparsePCA(alpha=0.8, batch_size=3, callback=None, method='lars',\n",
       "          n_components=6, n_iter=100, n_jobs=1,\n",
       "          random_state=<mtrand.RandomState object at 0x11cb0dd38>,\n",
       "          ridge_alpha=0.01, shuffle=True, verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(faces_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4096)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components_ = estimator.components_\n",
    "components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4096)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components_[:6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
