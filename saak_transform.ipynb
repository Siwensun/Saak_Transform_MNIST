{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from math import log\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "from skimage.util import *\n",
    "\n",
    "#load MNIST\n",
    "(X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_set_preprocessing(X, y, batch_ratio = 1, pad_size=2, pad_method='constant'):\n",
    "    X_pad = np.pad(X, ((0,0),(pad_size,pad_size),(pad_size,pad_size)), 'constant')\n",
    "    \n",
    "    batch_size = (int)(X_pad.shape[0]*batch_ratio)\n",
    "    \n",
    "    #order = np.array(range(X_pad.shape[0]))\n",
    "    #np.random.shuffle(order)\n",
    "    #X_pad_shuffle = X_pad[order]\n",
    "    #y_shuffle = y[order]\n",
    "    X_pad_shuffle = X_pad\n",
    "    y_shuffle = y\n",
    "\n",
    "    X_train_batch = ((X_pad_shuffle[0:batch_size, :, :]).astype('float32'))/255\n",
    "    y_train_batch = y_shuffle[0:batch_size,]\n",
    "    X_train_batch = X_train_batch.reshape(X_train_batch.shape[0], X_train_batch.shape[1], X_train_batch.shape[2], 1)\n",
    "    \n",
    "    return X_train_batch, y_train_batch\n",
    "\n",
    "def image_set_preprocessing_v2(X, y, batch_ratio = 1, pad_size=2, pad_method='constant'):\n",
    "    X_pad = np.pad(X, ((0,0),(pad_size,pad_size),(pad_size,pad_size)), 'constant')\n",
    "    \n",
    "    batch_size = (int)(X_pad.shape[0]*batch_ratio)\n",
    "    \n",
    "    order = np.array(range(X_pad.shape[0]))\n",
    "    np.random.shuffle(order)\n",
    "    X_pad_shuffle = X_pad[order]\n",
    "    y_shuffle = y[order]\n",
    "\n",
    "    X_train_batch = ((X_pad_shuffle[0:batch_size, :, :]).astype('float32'))/255\n",
    "    for i in range(X_train_batch.shape[0]):\n",
    "        X_train_batch[i,:,:] = random_noise(X_train_batch[i,:,:], clip=True)\n",
    "    y_train_batch = y_shuffle[0:batch_size,]\n",
    "    X_train_batch = X_train_batch.reshape(X_train_batch.shape[0], X_train_batch.shape[1], X_train_batch.shape[2], 1)\n",
    "    \n",
    "    return X_train_batch, y_train_batch\n",
    "\n",
    "def flatten_patches(cubio):\n",
    "    window_shape = (1,2,2,cubio.shape[3])\n",
    "    step = (1,2,2,cubio.shape[3])\n",
    "    patches = view_as_windows(cubio, window_shape, step)\n",
    "    patches = patches.squeeze(axis = (3,4))\n",
    "    patches_panel = patches.reshape(-1, patches.shape[-3]*patches.shape[-2]*patches.shape[-1])\n",
    "    \n",
    "    return patches_panel\n",
    "\n",
    "def remove_low_variance(patches_panel,thr=0.05):\n",
    "    var = np.var(patches_panel, axis = 1)\n",
    "    patches_clean = patches_panel[var>thr]\n",
    "    \n",
    "    return patches_clean\n",
    "\n",
    "def remove_patches_mean(patches):\n",
    "    mean = patches.mean(axis = 1)\n",
    "    patches_mean_remov = (patches.T-mean).T\n",
    "    \n",
    "    return patches_mean_remov\n",
    "\n",
    "def pca_kernel(patches, n_comps, kernel):\n",
    "    pca = PCA(n_components = n_comps)\n",
    "    pca.fit(patches)\n",
    "    kernel.append(pca)  \n",
    "    \n",
    "def pca_kernel_v2(patches, kernel):\n",
    "    pca = PCA()\n",
    "    pca.fit(patches)\n",
    "    kernel.append(pca)\n",
    "    \n",
    "def pca_kernel_v3(patches, n_comps, kernel):\n",
    "    pca = KernelPCA(n_components = n_comps)\n",
    "    pca.fit(patches)\n",
    "    kernel.append(pca)\n",
    "    \n",
    "def pca_transform(kernel, layer, patches_mean_remov, patches_panel, X_train):\n",
    "    n_sample = X_train.shape[0]\n",
    "    h = (int)(X_train.shape[1]/pow(2,layer))\n",
    "    patches_proj = k[layer-1].transform(patches_mean_remov)\n",
    "    cubio_pos = patches_proj.reshape(n_sample,h,h,-1)\n",
    "    cubio_neg = -cubio_pos\n",
    "    cubio_pca = np.concatenate((cubio_pos, cubio_neg), axis=3)\n",
    "    dc = patches_panel.mean(axis=1)*2\n",
    "    dc = dc.reshape(n_sample, h, h, -1)\n",
    "    cubio_next = np.concatenate((cubio_pca, dc), axis=3)\n",
    "    \n",
    "    return cubio_next\n",
    "\n",
    "def pca_transform_v2(kernel, layer, patches_mean_remov, patches_panel, X_train):\n",
    "    n_sample = X_train.shape[0]\n",
    "    h = (int)(X_train.shape[1]/pow(2,layer))\n",
    "    patches_proj = k[layer-1].transform(patches_mean_remov)\n",
    "    cubio_pos = patches_proj.reshape(n_sample,h,h,-1)\n",
    "    dc = patches_panel.mean(axis=1)*2\n",
    "    dc = dc.reshape(n_sample, h, h, -1)\n",
    "    cubio_next = np.concatenate((cubio_pos, dc), axis=3)\n",
    "    \n",
    "    return cubio_next\n",
    "\n",
    "def relu(cubio):\n",
    "    cubio_relu = cubio * (cubio > 0)\n",
    "    \n",
    "    return cubio_relu\n",
    "\n",
    "def tanh(cubio):\n",
    "    cubio_tanh = np.divide(np.exp(cubio), 1+np.exp(cubio))\n",
    "    \n",
    "    return cubio_tanh\n",
    "\n",
    "def one_stage_training(cubio, layer, n_comps, kernel, feature_list, X):\n",
    "    print(\"training at the %dth layer ...\" %(layer))\n",
    "    patches_panel = flatten_patches(cubio)\n",
    "    patches_mean_remov = remove_patches_mean(patches_panel)\n",
    "    if layer == 1:\n",
    "        patches_clean = remove_low_variance(patches_mean_remov)\n",
    "    else:\n",
    "        patches_clean = patches_mean_remov\n",
    "    pca_kernel(patches_clean,n_comps[layer-1], kernel)\n",
    "    cubio_next = relu(pca_transform(kernel, layer, patches_mean_remov, patches_panel, X))\n",
    "    feature_list.append(cubio_next)\n",
    "    print(\"Done! the shape of output cubio is %s.\" %(cubio_next.shape,))\n",
    "    \n",
    "    return cubio_next\n",
    "\n",
    "def one_stage_training_v2(cubio, layer, kernel, feature_list, X):\n",
    "    print(\"training at the %dth layer ...\" %(layer))\n",
    "    patches_panel = flatten_patches(cubio)\n",
    "    patches_mean_remov = remove_patches_mean(patches_panel)\n",
    "    if layer == 1:\n",
    "        patches_clean = remove_low_variance(patches_mean_remov)\n",
    "    else:\n",
    "        patches_clean = patches_mean_remov\n",
    "    pca_kernel_v2(patches_clean,kernel)\n",
    "    cubio_next = relu(pca_transform_v2(kernel, layer, patches_mean_remov, patches_panel, X))\n",
    "    feature_list.append(cubio_next)\n",
    "    print(\"Done! the shape of output cubio is %s.\" %(cubio_next.shape,))\n",
    "    \n",
    "    return cubio_next\n",
    "\n",
    "def one_stage_training_v3(cubio, layer, kernel, feature_list, X):\n",
    "    print(\"training at the %dth layer ...\" %(layer))\n",
    "    patches_panel = flatten_patches(cubio)\n",
    "    patches_mean_remov = remove_patches_mean(patches_panel)\n",
    "    if layer == 1:\n",
    "        patches_clean = remove_low_variance(patches_mean_remov)\n",
    "    else:\n",
    "        patches_clean = patches_mean_remov\n",
    "    pca_kernel(patches_clean,n_comps[layer-1],kernel)\n",
    "    cubio_next = tanh(pca_transform_v2(kernel, layer, patches_mean_remov, patches_panel, X))\n",
    "    feature_list.append(cubio_next)\n",
    "    print(\"Done! the shape of output cubio is %s.\" %(cubio_next.shape,))\n",
    "    \n",
    "    return cubio_next\n",
    "\n",
    "def feature_fusion(feature_list, num_layers):\n",
    "    feature = feature_list[0].reshape(feature_list[0].shape[0], -1)\n",
    "    for i in range(num_layers-1):\n",
    "        feature = np.concatenate((feature,feature_list[i+1].reshape(feature_list[i+1].shape[0], -1)), axis=1)\n",
    "    print(\"the shape of features we get is %s.\" %(feature.shape,))\n",
    "    return feature\n",
    "\n",
    "def Reduce_Feature(n_comps, feature):\n",
    "    pca = PCA(n_components = n_comps)\n",
    "    X_pc = pca.fit_transform(feature)\n",
    "    print(\"the number of dimensions kept is %d.\" %(X_pc.shape[1]))\n",
    "    \n",
    "    return X_pc, pca\n",
    "\n",
    "def Reduce_Feature_v2(n_comps, feature):\n",
    "    pca = KernelPCA(n_components = n_comps)\n",
    "    X_pc = pca.fit_transform(feature)\n",
    "    print(\"the number of dimensions kept is %d.\" %(X_pc.shape[1]))\n",
    "    \n",
    "    return X_pc, pca\n",
    "\n",
    "def F_test(percent, feature, label):\n",
    "    Ftest = SelectPercentile(chi2, percent)\n",
    "    X_f = Ftest.fit_transform(feature, label)\n",
    "    print(\"the number of feature dimensions passing F-test is %d.\" %(X_f.shape[1]))\n",
    "\n",
    "    return X_f, Ftest\n",
    "\n",
    "def RandomForest_FeatureSelect(percent, feature, label):\n",
    "    forest = RandomForestClassifier(random_state=0, )\n",
    "    forest.fit(feature, label)\n",
    "    index = np.argsort(forest.feature_importances_*100)\n",
    "    num = (int)((float)(feature.shape[1]*percent)/100)\n",
    "    X_rf = feature[:,index[0:num]]\n",
    "    print(\"the number of feature dimensions passing Random-Forest feature selection is %d.\" %(X_rf.shape[1]))\n",
    "    \n",
    "    return X_rf, forest, index\n",
    "\n",
    "def RandomForest_FeatureSelect_test(percent, feature, index):\n",
    "    num = (int)((float)(feature.shape[1]*percent)/100)\n",
    "    X_rf = feature[:,index[0:num]]\n",
    "    print(\"the number of feature dimensions passing Random-Forest feature selection is %d.\" %(X_rf.shape[1]))\n",
    "    \n",
    "    return X_rf\n",
    "    \n",
    "def SVM_training(feature, label, n_comps, percent):\n",
    "    print('SVM is under training...')\n",
    "    X_f, Ftest = F_test(percent, feature, label)\n",
    "    X_pc, pca = Reduce_Feature(n_comps, X_f)\n",
    "    clf = SVC()\n",
    "    clf.fit(X_pc, label)\n",
    "    y_pred = clf.predict(X_pc)\n",
    "    accuracy = accuracy_score(label, y_pred)\n",
    "    print(\"SVM accuracy on training sample is %f\" %(accuracy))\n",
    "    \n",
    "    return Ftest, pca, clf, accuracy\n",
    "\n",
    "def SVM_training_v2(X_pc, label):\n",
    "    print('SVM is under training...')\n",
    "    clf = SVC()\n",
    "    clf.fit(X_pc, label)\n",
    "    y_pred = clf.predict(X_pc)\n",
    "    accuracy = accuracy_score(label, y_pred)\n",
    "    print(\"SVM accuracy on training sample is %f\" %(accuracy))\n",
    "    \n",
    "    return clf, accuracy\n",
    "\n",
    "def RF_training(feature, label, n_comps, percent):\n",
    "    print('Random Forest is under training...')\n",
    "    X_f, Ftest = F_test(percent, feature, label)\n",
    "    X_pc, pca = Reduce_Feature(n_comps, X_f)\n",
    "    clf = RandomForestClassifier()  \n",
    "    clf.fit(X_pc, label)\n",
    "    y_pred = clf.predict(X_pc)\n",
    "    accuracy = accuracy_score(label, y_pred)\n",
    "    print(\"RF accuracy on training sample is %f\" %(accuracy))\n",
    "    \n",
    "    return Ftest, pca, clf, accuracy\n",
    "\n",
    "def RF_training_v2(X_pc, label):\n",
    "    print('Random Forest is under training...')\n",
    "    clf = RandomForestClassifier()  \n",
    "    clf.fit(X_pc, label)\n",
    "    y_pred = clf.predict(X_pc)\n",
    "    accuracy = accuracy_score(label, y_pred)\n",
    "    print(\"RF accuracy on training sample is %f\" %(accuracy))\n",
    "    \n",
    "    return clf, accuracy\n",
    "\n",
    "def one_stage_testing(cubio, layer, kernel, feature_list, X):\n",
    "    print(\"training at the %dth layer ...\" %(layer))\n",
    "    patches_panel = flatten_patches(cubio)\n",
    "    patches_mean_remov = remove_patches_mean(patches_panel)\n",
    "    cubio_next = relu(pca_transform(kernel, layer, patches_mean_remov, patches_panel, X))\n",
    "    feature_list.append(cubio_next)\n",
    "    print(\"Done! the shape of output cubio is %s.\" %(cubio_next.shape,))\n",
    "    \n",
    "    return cubio_next\n",
    "\n",
    "def one_stage_testing_v2(cubio, layer, kernel, feature_list, X):\n",
    "    print(\"training at the %dth layer ...\" %(layer))\n",
    "    patches_panel = flatten_patches(cubio)\n",
    "    patches_mean_remov = remove_patches_mean(patches_panel)\n",
    "    cubio_next = relu(pca_transform_v2(kernel, layer, patches_mean_remov, patches_panel, X))\n",
    "    feature_list.append(cubio_next)\n",
    "    print(\"Done! the shape of output cubio is %s.\" %(cubio_next.shape,))\n",
    "    \n",
    "    return cubio_next\n",
    "\n",
    "def one_stage_testing_v3(cubio, layer, kernel, feature_list, X):\n",
    "    print(\"training at the %dth layer ...\" %(layer))\n",
    "    patches_panel = flatten_patches(cubio)\n",
    "    patches_mean_remov = remove_patches_mean(patches_panel)\n",
    "    cubio_next = tanh(pca_transform_v2(kernel, layer, patches_mean_remov, patches_panel, X))\n",
    "    feature_list.append(cubio_next)\n",
    "    print(\"Done! the shape of output cubio is %s.\" %(cubio_next.shape,))\n",
    "    \n",
    "    return cubio_next\n",
    "\n",
    "def RF_testing(feature, label, Ftest, pca, clf):\n",
    "    print(\"Test smaples are under Random Forest's testing...\")\n",
    "    feature_test = Ftest.transform(feature)\n",
    "    print(\"the number of feature dimensions passing F-test is %d.\" %(feature_test.shape[1]))\n",
    "    feature_pca = pca.transform(feature_test)\n",
    "    print(\"the number of dimensions kept is %d.\" %(feature_pca.shape[1]))\n",
    "    y_pred = clf.predict(feature_pca)\n",
    "    accuracy = accuracy_score(label, y_pred)     \n",
    "    print(\"RF accuracy on test sample is %f\" %(accuracy))\n",
    "    \n",
    "    return accuracy, y_pred\n",
    "          \n",
    "def SVM_testing(feature, label, Ftest, pca, clf):\n",
    "    print(\"Test smaples are under SVM's testing...\")\n",
    "    feature_test = Ftest.transform(feature)\n",
    "    print(\"the number of feature dimensions passing F-test is %d.\" %(feature_test.shape[1]))\n",
    "    feature_pca = pca.transform(feature_test)\n",
    "    print(\"the number of dimensions kept is %d.\" %(feature_pca.shape[1]))\n",
    "    y_pred = clf.predict(feature_pca)\n",
    "    accuracy = accuracy_score(label, y_pred)     \n",
    "    print(\"SVM accuracy on test sample is %f\" %(accuracy))\n",
    "    \n",
    "    return accuracy, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented, ncomps controlled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training at the 1th layer ...\n",
      "Done! the shape of output cubio is (60000, 16, 16, 7).\n",
      "training at the 2th layer ...\n",
      "Done! the shape of output cubio is (60000, 8, 8, 9).\n",
      "training at the 3th layer ...\n",
      "Done! the shape of output cubio is (60000, 4, 4, 15).\n",
      "training at the 4th layer ...\n",
      "Done! the shape of output cubio is (60000, 2, 2, 13).\n",
      "training at the 5th layer ...\n",
      "Done! the shape of output cubio is (60000, 1, 1, 17).\n"
     ]
    }
   ],
   "source": [
    "#prepare training parameters list\n",
    "k = []\n",
    "feature_list = []\n",
    "n_comps = [3, 4, 7, 6, 8]\n",
    "\n",
    "#preprocess input image batch and label batch\n",
    "X_train, y_train_batch = image_set_preprocessing(X_train_raw, y_train_raw, batch_ratio = 1)\n",
    "cubio = copy.deepcopy(X_train)\n",
    "\n",
    "#compute the number of layers\n",
    "layer_cnt = (int)(log(X_train.shape[1])/log(2))\n",
    "\n",
    "#forward training process\n",
    "for layer in range(layer_cnt):\n",
    "    cubio = one_stage_training(cubio, layer+1, n_comps, k, feature_list, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of features we get is (60000, 2677).\n",
      "the number of feature dimensions passing F-test is 1338.\n",
      "the number of dimensions kept is 32.\n",
      "the number of dimensions kept is 64.\n",
      "the number of dimensions kept is 128.\n",
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.995583\n",
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.993933\n",
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.989367\n"
     ]
    }
   ],
   "source": [
    "#colloect all the features from all layers\n",
    "features = feature_fusion(feature_list, layer_cnt)\n",
    "\n",
    "#feature selection\n",
    "X_f, Ftest = F_test(50, features, y_train_batch)\n",
    "\n",
    "#feature reduction\n",
    "X_pc_32, pca_32 = Reduce_Feature(32, X_f)\n",
    "X_pc_64, pca_64 = Reduce_Feature(64, X_f)\n",
    "X_pc_128, pca_128 = Reduce_Feature(128, X_f)\n",
    "\n",
    "#SVM training\n",
    "svm_32, accuracy_svm_training_32= SVM_training_v2(X_pc_32, y_train_batch)\n",
    "svm_64, accuracy_svm_training_64= SVM_training_v2(X_pc_64, y_train_batch)\n",
    "svm_128, accuracy_svm_training_128= SVM_training_v2(X_pc_128, y_train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest is under training...\n",
      "RF accuracy on training sample is 0.998667\n",
      "Random Forest is under training...\n",
      "RF accuracy on training sample is 0.999033\n",
      "Random Forest is under training...\n",
      "RF accuracy on training sample is 0.998850\n"
     ]
    }
   ],
   "source": [
    "#Random Forest training\n",
    "rf_32, accuracy_rf_training_32 = RF_training_v2(X_pc_32, y_train_batch)\n",
    "rf_64, accuracy_rf_training_64 = RF_training_v2(X_pc_64, y_train_batch)\n",
    "rf_128, accuracy_rf_training_128 = RF_training_v2(X_pc_128, y_train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training at the 1th layer ...\n",
      "Done! the shape of output cubio is (10000, 16, 16, 7).\n",
      "training at the 2th layer ...\n",
      "Done! the shape of output cubio is (10000, 8, 8, 9).\n",
      "training at the 3th layer ...\n",
      "Done! the shape of output cubio is (10000, 4, 4, 15).\n",
      "training at the 4th layer ...\n",
      "Done! the shape of output cubio is (10000, 2, 2, 13).\n",
      "training at the 5th layer ...\n",
      "Done! the shape of output cubio is (10000, 1, 1, 17).\n",
      "the shape of features we get is (10000, 2677).\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 1338.\n",
      "the number of dimensions kept is 32.\n",
      "SVM accuracy on test sample is 0.981400\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 1338.\n",
      "the number of dimensions kept is 64.\n",
      "SVM accuracy on test sample is 0.983500\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 1338.\n",
      "the number of dimensions kept is 128.\n",
      "SVM accuracy on test sample is 0.981200\n",
      "Test smaples are under Random Forest's testing...\n",
      "the number of feature dimensions passing F-test is 1338.\n",
      "the number of dimensions kept is 32.\n",
      "RF accuracy on test sample is 0.927500\n",
      "Test smaples are under Random Forest's testing...\n",
      "the number of feature dimensions passing F-test is 1338.\n",
      "the number of dimensions kept is 64.\n",
      "RF accuracy on test sample is 0.923100\n",
      "Test smaples are under Random Forest's testing...\n",
      "the number of feature dimensions passing F-test is 1338.\n",
      "the number of dimensions kept is 128.\n",
      "RF accuracy on test sample is 0.905600\n"
     ]
    }
   ],
   "source": [
    "#preprocess testing image batch and label batch \n",
    "X_test, y_test_batch = image_set_preprocessing(X_test_raw, y_test_raw, batch_ratio = 1)\n",
    "\n",
    "#prepare testing parameters list\n",
    "feature_list_test = []\n",
    "\n",
    "#forward testing process\n",
    "cubio_test = copy.deepcopy(X_test)\n",
    "for layer in range(layer_cnt):\n",
    "    cubio_test = one_stage_testing(cubio_test, layer+1, k, feature_list_test, X_test)\n",
    "\n",
    "#colloect all the features from all layers\n",
    "features_test = feature_fusion(feature_list_test, layer_cnt)\n",
    "\n",
    "#F-test --> PCA reducing dims --> SVM testing\n",
    "accuracy_svm_testing_32, y_pred_svm_32 = SVM_testing(features_test, y_test_batch, Ftest, pca_32, svm_32)\n",
    "accuracy_svm_testing_64, y_pred_svm_64 = SVM_testing(features_test, y_test_batch, Ftest, pca_64, svm_64)\n",
    "accuracy_svm_testing_128, y_pred_svm_128 = SVM_testing(features_test, y_test_batch, Ftest, pca_128, svm_128)\n",
    "\n",
    "#F-test --> PCA reducing dims --> RF testing\n",
    "accuracy_rf_testing_32, y_pred_rf_32 = RF_testing(features_test, y_test_batch, Ftest, pca_32, rf_32)\n",
    "accuracy_rf_testing_64, y_pred_rf_64 = RF_testing(features_test, y_test_batch, Ftest, pca_64, rf_64)\n",
    "accuracy_rf_testing_128, y_pred_rf_128 = RF_testing(features_test, y_test_batch, Ftest, pca_128, rf_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training at the 1th layer ...\n",
      "Done! the shape of output cubio is (10000, 16, 16, 7).\n",
      "training at the 2th layer ...\n",
      "Done! the shape of output cubio is (10000, 8, 8, 9).\n",
      "training at the 3th layer ...\n",
      "Done! the shape of output cubio is (10000, 4, 4, 15).\n",
      "training at the 4th layer ...\n",
      "Done! the shape of output cubio is (10000, 2, 2, 13).\n",
      "training at the 5th layer ...\n",
      "Done! the shape of output cubio is (10000, 1, 1, 17).\n",
      "the shape of features we get is (10000, 2677).\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 1338.\n",
      "the number of dimensions kept is 64.\n",
      "SVM accuracy on test sample is 0.983500\n",
      "[7 2 1 ... 4 5 6]\n",
      "[7 2 1 ... 4 5 6]\n",
      "[[ 973    0    1    0    0    2    1    1    2    0]\n",
      " [   0 1129    3    0    0    1    0    0    1    1]\n",
      " [   5    0 1013    1    1    0    1    7    4    0]\n",
      " [   0    0    2  996    1    3    0    4    3    1]\n",
      " [   0    0    3    0  963    0    4    0    1   11]\n",
      " [   2    0    0    8    1  873    2    1    3    2]\n",
      " [   5    2    0    0    2    3  945    0    1    0]\n",
      " [   0    4    7    0    1    0    0 1008    0    8]\n",
      " [   3    0    1    4    2    1    1    2  957    3]\n",
      " [   2    4    0    7    9    2    1    4    2  978]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9828    0.9929    0.9878       980\n",
      "          1     0.9912    0.9947    0.9930      1135\n",
      "          2     0.9835    0.9816    0.9825      1032\n",
      "          3     0.9803    0.9861    0.9832      1010\n",
      "          4     0.9827    0.9807    0.9817       982\n",
      "          5     0.9864    0.9787    0.9826       892\n",
      "          6     0.9895    0.9864    0.9880       958\n",
      "          7     0.9815    0.9805    0.9810      1028\n",
      "          8     0.9825    0.9825    0.9825       974\n",
      "          9     0.9741    0.9693    0.9717      1009\n",
      "\n",
      "avg / total     0.9835    0.9835    0.9835     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#preprocess testing image batch and label batch \n",
    "X_test, y_test_batch = image_set_preprocessing(X_test_raw, y_test_raw, batch_ratio = 1)\n",
    "\n",
    "#prepare testing parameters list\n",
    "feature_list_test = []\n",
    "\n",
    "#forward testing process\n",
    "cubio_test = copy.deepcopy(X_test)\n",
    "for layer in range(layer_cnt):\n",
    "    cubio_test = one_stage_testing(cubio_test, layer+1, k, feature_list_test, X_test)\n",
    "\n",
    "#colloect all the features from all layers\n",
    "features_test = feature_fusion(feature_list_test, layer_cnt)\n",
    "\n",
    "#F-test --> PCA reducing dims --> SVM testing\n",
    "accuracy_svm_testing_64, y_pred_svm_64 = SVM_testing(features_test, y_test_batch, Ftest, pca_64, svm_64)\n",
    "\n",
    "print(y_pred_svm_64)\n",
    "print(y_test_batch)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test_batch, y_pred_svm_64, labels = [0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "rp = classification_report(y_test_batch, y_pred_svm_64, digits = 4)\n",
    "\n",
    "print(cm)\n",
    "print(rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.array([ 321,  447,  659,  740,  882,  947, 1014, 1112, 1232, 1242, 1247,\n",
    "       1527, 1709, 1790, 1878, 1901, 2018, 2129, 2130, 2135, 2182, 2266,\n",
    "       2293, 2597, 2654, 2896, 2939, 3073, 3225, 3422, 3534, 3558, 3727,\n",
    "       3941, 4176, 4201, 4284, 4443, 4507, 4571, 4699, 4740, 4761, 4783,\n",
    "       4860, 4879, 5937, 5955, 6576, 6597, 6625, 6651, 7216, 8246, 8527,\n",
    "       9642, 9664, 9692, 9729])\n",
    "y_diff = y_pred_svm_64-y_test_batch\n",
    "index_2 = y_diff.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 151,  247,  321,  340,  381,  445,  447,  448,  449,  495,  582,\n",
       "         659,  689,  691,  707,  740,  882,  947,  951,  956,  965, 1014,\n",
       "        1039, 1044, 1112, 1192, 1226, 1232, 1242, 1247, 1260, 1299, 1319,\n",
       "        1328, 1393, 1494, 1500, 1522, 1530, 1549, 1553, 1581, 1609, 1681,\n",
       "        1709, 1790, 1901, 2024, 2044, 2070, 2098, 2109, 2118, 2130, 2135,\n",
       "        2182, 2185, 2189, 2224, 2272, 2293, 2299, 2371, 2387, 2406, 2414,\n",
       "        2422, 2462, 2488, 2597, 2607, 2648, 2654, 2771, 2810, 2863, 2896,\n",
       "        2927, 2939, 2953, 3060, 3073, 3117, 3405, 3422, 3475, 3503, 3520,\n",
       "        3558, 3559, 3597, 3751, 3767, 3776, 3780, 3796, 3808, 3811, 3853,\n",
       "        3902, 3941, 3985, 4065, 4075, 4078, 4163, 4176, 4201, 4224, 4248,\n",
       "        4271, 4289, 4306, 4497, 4500, 4575, 4740, 4807, 4823, 4879, 4880,\n",
       "        4966, 5457, 5642, 5734, 5936, 5937, 5955, 5973, 5985, 5997, 6035,\n",
       "        6059, 6166, 6555, 6560, 6571, 6576, 6597, 6598, 6625, 6651, 6755,\n",
       "        7434, 8094, 8325, 8339, 8527, 9009, 9015, 9024, 9587, 9634, 9664,\n",
       "        9679, 9698, 9729, 9745, 9749, 9768, 9770, 9779, 9792, 9808, 9944]),)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 165)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(np.setdiff1d(index, index_2)))\n",
    "#print(len(np.setdiff1d(index_2, index)))\n",
    "#print(len(index))\n",
    "index_2 = np.array(index_2)\n",
    "index_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# without augmented, without comps controlled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training at the 1th layer ...\n",
      "Done! the shape of output cubio is (60000, 16, 16, 5).\n",
      "training at the 2th layer ...\n",
      "Done! the shape of output cubio is (60000, 8, 8, 21).\n",
      "training at the 3th layer ...\n",
      "Done! the shape of output cubio is (60000, 4, 4, 85).\n",
      "training at the 4th layer ...\n",
      "Done! the shape of output cubio is (60000, 2, 2, 341).\n",
      "training at the 5th layer ...\n",
      "Done! the shape of output cubio is (60000, 1, 1, 1365).\n"
     ]
    }
   ],
   "source": [
    "#prepare training parameters list\n",
    "k = []\n",
    "feature_list = []\n",
    "\n",
    "#preprocess input image batch and label batch\n",
    "X_train, y_train_batch = image_set_preprocessing(X_train_raw, y_train_raw, batch_ratio = 1)\n",
    "cubio = copy.deepcopy(X_train)\n",
    "\n",
    "#compute the number of layers\n",
    "layer_cnt = (int)(log(X_train.shape[1])/log(2))\n",
    "\n",
    "#forward training process\n",
    "for layer in range(layer_cnt):\n",
    "    cubio = one_stage_training_v2(cubio, layer+1, k, feature_list, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a16ef2da0>"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(feature_list[0][0,:,:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a15a9b470>"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2677)"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## only use the final layer output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM is under training...\n",
      "the number of feature dimensions passing F-test is 682.\n",
      "the number of dimensions kept is 32.\n",
      "SVM accuracy on training sample is 0.787700\n",
      "SVM is under training...\n",
      "the number of feature dimensions passing F-test is 682.\n",
      "the number of dimensions kept is 64.\n",
      "SVM accuracy on training sample is 0.821700\n",
      "SVM is under training...\n",
      "the number of feature dimensions passing F-test is 682.\n",
      "the number of dimensions kept is 128.\n",
      "SVM accuracy on training sample is 0.818467\n",
      "Random Forest is under training...\n",
      "the number of feature dimensions passing F-test is 682.\n",
      "the number of dimensions kept is 32.\n",
      "RF accuracy on training sample is 0.996767\n",
      "Random Forest is under training...\n",
      "the number of feature dimensions passing F-test is 682.\n",
      "the number of dimensions kept is 64.\n",
      "RF accuracy on training sample is 0.997283\n",
      "Random Forest is under training...\n",
      "the number of feature dimensions passing F-test is 682.\n",
      "the number of dimensions kept is 128.\n",
      "RF accuracy on training sample is 0.997667\n"
     ]
    }
   ],
   "source": [
    "features = cubio.squeeze()\n",
    "#F-test --> PCA reducing dims --> SVM training\n",
    "Ftest, pca_32, svm_32, accuracy_svm_training_32= SVM_training(features, y_train_batch, 32, 50)\n",
    "Ftest, pca_64, svm_64, accuracy_svm_training_64= SVM_training(features, y_train_batch, 64, 50)\n",
    "Ftest, pca_128, svm_128, accuracy_svm_training_128= SVM_training(features, y_train_batch, 128, 50)\n",
    "\n",
    "#F-test --> PCA reducing dims --> RF training\n",
    "Ftest, pca_32, rf_32, accuracy_rf_training_32 = RF_training(features, y_train_batch, 32, 50)\n",
    "Ftest, pca_64, rf_64, accuracy_rf_training_64 = RF_training(features, y_train_batch, 64, 50)\n",
    "Ftest, pca_128, rf_128, accuracy_rf_training_128 = RF_training(features, y_train_batch, 128, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training at the 1th layer ...\n",
      "Done! the shape of output cubio is (10000, 16, 16, 5).\n",
      "training at the 2th layer ...\n",
      "Done! the shape of output cubio is (10000, 8, 8, 21).\n",
      "training at the 3th layer ...\n",
      "Done! the shape of output cubio is (10000, 4, 4, 85).\n",
      "training at the 4th layer ...\n",
      "Done! the shape of output cubio is (10000, 2, 2, 341).\n",
      "training at the 5th layer ...\n",
      "Done! the shape of output cubio is (10000, 1, 1, 1365).\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 682.\n",
      "the number of dimensions kept is 32.\n",
      "SVM accuracy on test sample is 0.790900\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 682.\n",
      "the number of dimensions kept is 64.\n",
      "SVM accuracy on test sample is 0.815500\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 682.\n",
      "the number of dimensions kept is 128.\n",
      "SVM accuracy on test sample is 0.816800\n",
      "Test smaples are under Random Forest's testing...\n",
      "the number of feature dimensions passing F-test is 682.\n",
      "the number of dimensions kept is 32.\n",
      "RF accuracy on test sample is 0.809100\n",
      "Test smaples are under Random Forest's testing...\n",
      "the number of feature dimensions passing F-test is 682.\n",
      "the number of dimensions kept is 64.\n",
      "RF accuracy on test sample is 0.803400\n",
      "Test smaples are under Random Forest's testing...\n",
      "the number of feature dimensions passing F-test is 682.\n",
      "the number of dimensions kept is 128.\n",
      "RF accuracy on test sample is 0.784100\n"
     ]
    }
   ],
   "source": [
    "#preprocess testing image batch and label batch \n",
    "X_test, y_test_batch = image_set_preprocessing(X_test_raw, y_test_raw, batch_ratio = 1)\n",
    "\n",
    "#prepare testing parameters list\n",
    "feature_list_test = []\n",
    "\n",
    "#forward testing process\n",
    "cubio_test = copy.deepcopy(X_test)\n",
    "for layer in range(layer_cnt):\n",
    "    cubio_test = one_stage_testing_v2(cubio_test, layer+1, k, feature_list_test, X_test)\n",
    "\n",
    "features_test = cubio_test.squeeze()\n",
    "\n",
    "#F-test --> PCA reducing dims --> SVM testing\n",
    "accuracy_svm_testing_32= SVM_testing(features_test, y_test_batch, Ftest, pca_32, svm_32)\n",
    "accuracy_svm_testing_64= SVM_testing(features_test, y_test_batch, Ftest, pca_64, svm_64)\n",
    "accuracy_svm_testing_128= SVM_testing(features_test, y_test_batch, Ftest, pca_128, svm_128)\n",
    "\n",
    "#F-test --> PCA reducing dims --> RF testing\n",
    "accuracy_rf_testing_32 = RF_testing(features_test, y_test_batch, Ftest, pca_32, rf_32)\n",
    "accuracy_rf_testing_64 = RF_testing(features_test, y_test_batch, Ftest, pca_64, rf_64)\n",
    "accuracy_rf_testing_128 = RF_testing(features_test, y_test_batch, Ftest, pca_128, rf_128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using features from all the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of features we get is (60000, 6713).\n",
      "SVM is under training...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 32.\n",
      "SVM accuracy on training sample is 0.994133\n",
      "SVM is under training...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 64.\n",
      "SVM accuracy on training sample is 0.991683\n",
      "SVM is under training...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 128.\n",
      "SVM accuracy on training sample is 0.986333\n",
      "Random Forest is under training...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 32.\n",
      "RF accuracy on training sample is 0.998767\n",
      "Random Forest is under training...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 64.\n",
      "RF accuracy on training sample is 0.999017\n",
      "Random Forest is under training...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 128.\n",
      "RF accuracy on training sample is 0.999233\n"
     ]
    }
   ],
   "source": [
    "#colloect all the features from all layers\n",
    "features = feature_fusion(feature_list, layer_cnt)\n",
    "\n",
    "#F-test --> PCA reducing dims --> SVM training\n",
    "Ftest, pca_32, svm_32, accuracy_svm_training_32= SVM_training(features, y_train_batch, 32, 50)\n",
    "Ftest, pca_64, svm_64, accuracy_svm_training_64= SVM_training(features, y_train_batch, 64, 50)\n",
    "Ftest, pca_128, svm_128, accuracy_svm_training_128= SVM_training(features, y_train_batch, 128, 50)\n",
    "\n",
    "#F-test --> PCA reducing dims --> RF training\n",
    "Ftest, pca_32, rf_32, accuracy_rf_training_32 = RF_training(features, y_train_batch, 32, 50)\n",
    "Ftest, pca_64, rf_64, accuracy_rf_training_64 = RF_training(features, y_train_batch, 64, 50)\n",
    "Ftest, pca_128, rf_128, accuracy_rf_training_128 = RF_training(features, y_train_batch, 128, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of features we get is (10000, 6713).\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 32.\n",
      "SVM accuracy on test sample is 0.981500\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 64.\n",
      "SVM accuracy on test sample is 0.981200\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 128.\n",
      "SVM accuracy on test sample is 0.978700\n",
      "Test smaples are under Random Forest's testing...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 32.\n",
      "RF accuracy on test sample is 0.925500\n",
      "Test smaples are under Random Forest's testing...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 64.\n",
      "RF accuracy on test sample is 0.922500\n",
      "Test smaples are under Random Forest's testing...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 128.\n",
      "RF accuracy on test sample is 0.903100\n"
     ]
    }
   ],
   "source": [
    "#colloect all the features from all layers\n",
    "features_test = feature_fusion(feature_list_test, layer_cnt)\n",
    "\n",
    "#F-test --> PCA reducing dims --> SVM testing\n",
    "accuracy_svm_testing_32= SVM_testing(features_test, y_test_batch, Ftest, pca_32, svm_32)\n",
    "accuracy_svm_testing_64= SVM_testing(features_test, y_test_batch, Ftest, pca_64, svm_64)\n",
    "accuracy_svm_testing_128= SVM_testing(features_test, y_test_batch, Ftest, pca_128, svm_128)\n",
    "\n",
    "#F-test --> PCA reducing dims --> RF testing\n",
    "accuracy_rf_testing_32 = RF_testing(features_test, y_test_batch, Ftest, pca_32, rf_32)\n",
    "accuracy_rf_testing_64 = RF_testing(features_test, y_test_batch, Ftest, pca_64, rf_64)\n",
    "accuracy_rf_testing_128 = RF_testing(features_test, y_test_batch, Ftest, pca_128, rf_128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Noise to training images, without augments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training at the 1th layer ...\n",
      "Done! the shape of output cubio is (60000, 16, 16, 5).\n",
      "training at the 2th layer ...\n",
      "Done! the shape of output cubio is (60000, 8, 8, 21).\n",
      "training at the 3th layer ...\n",
      "Done! the shape of output cubio is (60000, 4, 4, 85).\n",
      "training at the 4th layer ...\n",
      "Done! the shape of output cubio is (60000, 2, 2, 341).\n",
      "training at the 5th layer ...\n",
      "Done! the shape of output cubio is (60000, 1, 1, 1365).\n"
     ]
    }
   ],
   "source": [
    "#prepare training parameters list\n",
    "k = []\n",
    "feature_list = []\n",
    "n_comps = [3, 4, 7, 6, 8]\n",
    "\n",
    "#preprocess input image batch and label batch\n",
    "X_train, y_train_batch = image_set_preprocessing_v2(X_train_raw, y_train_raw, batch_ratio = 1)\n",
    "cubio = copy.deepcopy(X_train)\n",
    "\n",
    "#compute the number of layers\n",
    "layer_cnt = (int)(log(X_train.shape[1])/log(2))\n",
    "\n",
    "#forward training process\n",
    "for layer in range(layer_cnt):\n",
    "    cubio = one_stage_training_v2(cubio, layer+1, k, feature_list, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of features we get is (60000, 6713).\n",
      "SVM is under training...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 32.\n",
      "SVM accuracy on training sample is 0.992817\n",
      "SVM is under training...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 64.\n",
      "SVM accuracy on training sample is 0.990100\n",
      "SVM is under training...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 128.\n",
      "SVM accuracy on training sample is 0.983633\n",
      "Random Forest is under training...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 32.\n",
      "RF accuracy on training sample is 0.998883\n",
      "Random Forest is under training...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 64.\n",
      "RF accuracy on training sample is 0.998817\n",
      "Random Forest is under training...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 128.\n",
      "RF accuracy on training sample is 0.999233\n"
     ]
    }
   ],
   "source": [
    "#colloect all the features from all layers\n",
    "features = feature_fusion(feature_list, layer_cnt)\n",
    "\n",
    "#F-test --> PCA reducing dims --> SVM training\n",
    "Ftest, pca_32, svm_32, accuracy_svm_training_32= SVM_training(features, y_train_batch, 32, 50)\n",
    "Ftest, pca_64, svm_64, accuracy_svm_training_64= SVM_training(features, y_train_batch, 64, 50)\n",
    "Ftest, pca_128, svm_128, accuracy_svm_training_128= SVM_training(features, y_train_batch, 128, 50)\n",
    "\n",
    "#F-test --> PCA reducing dims --> RF training\n",
    "Ftest, pca_32, rf_32, accuracy_rf_training_32 = RF_training(features, y_train_batch, 32, 50)\n",
    "Ftest, pca_64, rf_64, accuracy_rf_training_64 = RF_training(features, y_train_batch, 64, 50)\n",
    "Ftest, pca_128, rf_128, accuracy_rf_training_128 = RF_training(features, y_train_batch, 128, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training at the 1th layer ...\n",
      "Done! the shape of output cubio is (10000, 16, 16, 5).\n",
      "training at the 2th layer ...\n",
      "Done! the shape of output cubio is (10000, 8, 8, 21).\n",
      "training at the 3th layer ...\n",
      "Done! the shape of output cubio is (10000, 4, 4, 85).\n",
      "training at the 4th layer ...\n",
      "Done! the shape of output cubio is (10000, 2, 2, 341).\n",
      "training at the 5th layer ...\n",
      "Done! the shape of output cubio is (10000, 1, 1, 1365).\n",
      "the shape of features we get is (10000, 6713).\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 32.\n",
      "SVM accuracy on test sample is 0.980100\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 64.\n",
      "SVM accuracy on test sample is 0.980200\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 128.\n",
      "SVM accuracy on test sample is 0.975600\n",
      "Test smaples are under Random Forest's testing...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 32.\n",
      "RF accuracy on test sample is 0.924800\n",
      "Test smaples are under Random Forest's testing...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 64.\n",
      "RF accuracy on test sample is 0.914400\n",
      "Test smaples are under Random Forest's testing...\n",
      "the number of feature dimensions passing F-test is 3356.\n",
      "the number of dimensions kept is 128.\n",
      "RF accuracy on test sample is 0.892400\n"
     ]
    }
   ],
   "source": [
    "#preprocess testing image batch and label batch \n",
    "X_test, y_test_batch = image_set_preprocessing(X_test_raw, y_test_raw, batch_ratio = 1)\n",
    "\n",
    "#prepare testing parameters list\n",
    "feature_list_test = []\n",
    "\n",
    "#forward testing process\n",
    "cubio_test = copy.deepcopy(X_test)\n",
    "for layer in range(layer_cnt):\n",
    "    cubio_test = one_stage_testing_v2(cubio_test, layer+1, k, feature_list_test, X_test)\n",
    "\n",
    "#colloect all the features from all layers\n",
    "features_test = feature_fusion(feature_list_test, layer_cnt)\n",
    "\n",
    "#F-test --> PCA reducing dims --> SVM testing\n",
    "accuracy_svm_testing_32= SVM_testing(features_test, y_test_batch, Ftest, pca_32, svm_32)\n",
    "accuracy_svm_testing_64= SVM_testing(features_test, y_test_batch, Ftest, pca_64, svm_64)\n",
    "accuracy_svm_testing_128= SVM_testing(features_test, y_test_batch, Ftest, pca_128, svm_128)\n",
    "\n",
    "#F-test --> PCA reducing dims --> RF testing\n",
    "accuracy_rf_testing_32 = RF_testing(features_test, y_test_batch, Ftest, pca_32, rf_32)\n",
    "accuracy_rf_testing_64 = RF_testing(features_test, y_test_batch, Ftest, pca_64, rf_64)\n",
    "accuracy_rf_testing_128 = RF_testing(features_test, y_test_batch, Ftest, pca_128, rf_128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sigmoid as the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training at the 1th layer ...\n",
      "Done! the shape of output cubio is (60000, 16, 16, 4).\n",
      "training at the 2th layer ...\n",
      "Done! the shape of output cubio is (60000, 8, 8, 5).\n",
      "training at the 3th layer ...\n",
      "Done! the shape of output cubio is (60000, 4, 4, 8).\n",
      "training at the 4th layer ...\n",
      "Done! the shape of output cubio is (60000, 2, 2, 7).\n",
      "training at the 5th layer ...\n",
      "Done! the shape of output cubio is (60000, 1, 1, 9).\n"
     ]
    }
   ],
   "source": [
    "#prepare training parameters list\n",
    "k = []\n",
    "feature_list = []\n",
    "n_comps = [3, 4, 7, 6, 8]\n",
    "\n",
    "#preprocess input image batch and label batch\n",
    "X_train, y_train_batch = image_set_preprocessing_v2(X_train_raw, y_train_raw, batch_ratio = 1)\n",
    "cubio = copy.deepcopy(X_train)\n",
    "\n",
    "#compute the number of layers\n",
    "layer_cnt = (int)(log(X_train.shape[1])/log(2))\n",
    "\n",
    "#forward training process\n",
    "for layer in range(layer_cnt):\n",
    "    cubio = one_stage_training_v3(cubio, layer+1, k, feature_list, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of features we get is (60000, 1509).\n"
     ]
    }
   ],
   "source": [
    "#colloect all the features from all layers\n",
    "features = feature_fusion(feature_list, layer_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of feature dimensions passing F-test is 754.\n",
      "the number of dimensions kept is 32.\n",
      "the number of dimensions kept is 64.\n",
      "the number of dimensions kept is 128.\n"
     ]
    }
   ],
   "source": [
    "X_f, Ftest = F_test(50, features, y_train_batch)\n",
    "X_pc_32, pca_32 = Reduce_Feature(32, X_f)\n",
    "X_pc_64, pca_64 = Reduce_Feature(64, X_f)\n",
    "X_pc_128, pca_128 = Reduce_Feature(128, X_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.924683\n",
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.926883\n",
      "SVM is under training...\n",
      "SVM accuracy on training sample is 0.921017\n"
     ]
    }
   ],
   "source": [
    "#F-test --> PCA reducing dims --> SVM training\n",
    "svm_32, accuracy_svm_training_32= SVM_training_v2(X_pc_32, y_train_batch)\n",
    "svm_64, accuracy_svm_training_64= SVM_training_v2(X_pc_64, y_train_batch)\n",
    "svm_128, accuracy_svm_training_128= SVM_training_v2(X_pc_128, y_train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training at the 1th layer ...\n",
      "Done! the shape of output cubio is (10000, 16, 16, 4).\n",
      "training at the 2th layer ...\n",
      "Done! the shape of output cubio is (10000, 8, 8, 5).\n",
      "training at the 3th layer ...\n",
      "Done! the shape of output cubio is (10000, 4, 4, 8).\n",
      "training at the 4th layer ...\n",
      "Done! the shape of output cubio is (10000, 2, 2, 7).\n",
      "training at the 5th layer ...\n",
      "Done! the shape of output cubio is (10000, 1, 1, 9).\n"
     ]
    }
   ],
   "source": [
    "#preprocess testing image batch and label batch \n",
    "X_test, y_test_batch = image_set_preprocessing(X_test_raw, y_test_raw, batch_ratio = 1)\n",
    "\n",
    "#prepare testing parameters list\n",
    "feature_list_test = []\n",
    "\n",
    "#forward testing process\n",
    "cubio_test = copy.deepcopy(X_test)\n",
    "for layer in range(layer_cnt):\n",
    "    cubio_test = one_stage_testing_v3(cubio_test, layer+1, k, feature_list_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of features we get is (10000, 1509).\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 754.\n",
      "the number of dimensions kept is 32.\n",
      "SVM accuracy on test sample is 0.927700\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 754.\n",
      "the number of dimensions kept is 64.\n",
      "SVM accuracy on test sample is 0.928600\n",
      "Test smaples are under SVM's testing...\n",
      "the number of feature dimensions passing F-test is 754.\n",
      "the number of dimensions kept is 128.\n",
      "SVM accuracy on test sample is 0.923400\n"
     ]
    }
   ],
   "source": [
    "#colloect all the features from all layers\n",
    "features_test = feature_fusion(feature_list_test, layer_cnt)\n",
    "\n",
    "#F-test --> PCA reducing dims --> SVM testing\n",
    "accuracy_svm_testing_32= SVM_testing(features_test, y_test_batch, Ftest, pca_32, svm_32)\n",
    "accuracy_svm_testing_64= SVM_testing(features_test, y_test_batch, Ftest, pca_64, svm_64)\n",
    "accuracy_svm_testing_128= SVM_testing(features_test, y_test_batch, Ftest, pca_128, svm_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1509)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
